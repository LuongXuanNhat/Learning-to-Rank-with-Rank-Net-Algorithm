{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6h7KPFJqNMB",
        "outputId": "6857e162-b2a2-4ef6-aaf7-2645cf431b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-94b50cf1-dccd-03a1-ee83-9e458e7f4cfe)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import các thư viện cần sử dụng"
      ],
      "metadata": {
        "id": "yLS4WrSarXBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-GwQPsx2mEg",
        "outputId": "19cec9fb-8da5-48ae-f518-c1ee79605d93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.6.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.55.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.8.3)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "from typing import List, Dict\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pyvi import ViTokenizer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "baNRYvdKrrKf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Định nghĩa class RankNet - kết thừa từ thư viện: torch.nn.Module"
      ],
      "metadata": {
        "id": "i4FErFmyv87U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128, dropout=0.2):\n",
        "        \"\"\"\n",
        "        RankNet với 2 tầng ẩn\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Số features đầu vào\n",
        "            hidden_size1 (int): Số neurons tầng ẩn thứ nhất\n",
        "            hidden_size2 (int): Số neurons tầng ẩn thứ hai\n",
        "            dropout (float): Tỷ lệ dropout để tránh overfitting\n",
        "        \"\"\"\n",
        "        super(RankNet, self).__init__()\n",
        "\n",
        "        # Định nghĩa các tầng\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer cho score\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Layer normalization thay vì batch normalization để tránh lỗi khi batch_size=1\n",
        "        self.ln1 = nn.LayerNorm(hidden_size1)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor có shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output scores có shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Đảm bảo input là tensor và có đúng kiểu dữ liệu\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        # Tầng ẩn thứ nhất\n",
        "        x = self.fc1(x)\n",
        "        x = self.ln1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Tầng ẩn thứ hai\n",
        "        x = self.fc2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict_rank(self, x1, x2):\n",
        "        \"\"\"\n",
        "        So sánh ranking giữa hai samples\n",
        "\n",
        "        Args:\n",
        "            x1, x2 (torch.Tensor): Hai samples cần so sánh\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Xác suất x1 được rank cao hơn x2\n",
        "        \"\"\"\n",
        "        score1 = self.forward(x1)\n",
        "        score2 = self.forward(x2)\n",
        "\n",
        "        # Sử dụng sigmoid để chuyển về xác suất\n",
        "        prob = torch.sigmoid(score1 - score2)\n",
        "        return prob\n"
      ],
      "metadata": {
        "id": "OPdkzRkzwIcW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm mất mát"
      ],
      "metadata": {
        "id": "Zkp0aCK-w58i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ranknet_loss(s_i, s_j, P_ij):\n",
        "    diff = s_i - s_j\n",
        "    P_hat = torch.sigmoid(diff)  # Xác suất dự đoán P̂ᵢⱼ\n",
        "    # Thêm epsilon để tránh log(0)\n",
        "    epsilon = 1e-10\n",
        "    loss = -P_ij * torch.log(P_hat + epsilon) - (1 - P_ij) * torch.log(1 - P_hat + epsilon)\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "id": "D3YdLQ86w77_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Một số lớp, hàm phụ trợ"
      ],
      "metadata": {
        "id": "c4zlhnATyWha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Truy xuất dữ liệu\n",
        "class DocumentRetriever:\n",
        "    def __init__(self, documents, sentence_model, stopwords_path=None):\n",
        "        self.documents = documents\n",
        "        self.model = sentence_model\n",
        "        self.stopwords = set()\n",
        "\n",
        "        # Load stopwords nếu có\n",
        "        if stopwords_path and os.path.exists(stopwords_path):\n",
        "            with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "                self.stopwords = set(line.strip() for line in f if line.strip())\n",
        "\n",
        "    # Làm sạch dữ liệu văn bản thuần\n",
        "    def preprocess(self, text):\n",
        "        \"\"\"\n",
        "        [Đã nâng cấp] Sử dụng PyVi\n",
        "        Tách từ đơn giản (theo khoảng trắng) và loại bỏ stopword.\n",
        "        Args:\n",
        "            text (str): Câu đầu vào.\n",
        "        Returns:\n",
        "            List[str]: Danh sách từ đã lọc stopword.\n",
        "        \"\"\"\n",
        "        # 1. Chuẩn hóa: bỏ dấu câu, ký tự đặc biệt, số (nếu cần)\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)  # loại bỏ dấu câu\n",
        "        text = re.sub(r'\\d+', ' ', text)      # loại bỏ số\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # bỏ khoảng trắng thừa\n",
        "\n",
        "        # 2. Tokenize với PyVi\n",
        "        tokenized = ViTokenizer.tokenize(text) # từ đơn, từ ghép, cụm từ cố định | Ông Nguyễn_Tấn_Dũng là cựu_thủ_tướng Việt_Nam .\n",
        "\n",
        "        # 3. Tách thành danh sách từ\n",
        "        tokens = tokenized.split()\n",
        "\n",
        "        if self.stopwords:\n",
        "            tokens = [w for w in tokens if w not in self.stopwords]\n",
        "\n",
        "        return tokens\n",
        "\n",
        "    # Tạo cặp dữ liệu huấn luyện\n",
        "    def generate_pairwise_training_data(self, queries: List[str], documents: List[Dict]) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Tạo dữ liệu pairwise training cho RankNet\n",
        "        Mỗi sample là một cặp (query, doc1, doc2, label)\n",
        "        label = 1 nếu doc1 relevance > doc2, ngược lại = 0\n",
        "        \"\"\"\n",
        "        pairwise_data = []\n",
        "\n",
        "        for query in tqdm(queries, desc=\"Tạo dữ liệu pairwise\"):\n",
        "            # Preprocess query\n",
        "            processed_query_tokens = self.preprocess(query)\n",
        "            processed_query = \" \".join(processed_query_tokens)\n",
        "\n",
        "            # Tạo embeddings\n",
        "            query_embedding = self.model.encode([processed_query])\n",
        "            doc_texts = []\n",
        "            for doc in documents:\n",
        "                processed_doc_tokens = self.preprocess(doc['value'])\n",
        "                processed_doc = \" \".join(processed_doc_tokens)\n",
        "                doc_texts.append(processed_doc)\n",
        "\n",
        "            doc_embeddings = self.model.encode(doc_texts)\n",
        "\n",
        "            # Tính cosine similarity\n",
        "            similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "            # Gán điểm relevance dựa trên similarity\n",
        "            doc_relevances = []\n",
        "            doc_similarities = [(i, sim) for i, sim in enumerate(similarities)]\n",
        "            doc_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for rank, (doc_idx, similarity) in enumerate(doc_similarities):\n",
        "                if rank < 3:  # Top 1-3\n",
        "                    relevance = 3\n",
        "                elif rank < 7:  # Top 4-7\n",
        "                    relevance = 2\n",
        "                elif rank < 12:  # Top 8-12\n",
        "                    relevance = 1\n",
        "                else:  # Còn lại\n",
        "                    relevance = 0\n",
        "\n",
        "                doc_relevances.append({\n",
        "                    'doc_idx': doc_idx,\n",
        "                    'relevance': relevance,\n",
        "                    'similarity': similarity,\n",
        "                    'document': documents[doc_idx]\n",
        "                })\n",
        "\n",
        "            # Tạo tất cả các cặp document có relevance khác nhau\n",
        "            for i in range(len(doc_relevances)):\n",
        "                for j in range(i + 1, len(doc_relevances)):\n",
        "                    doc1 = doc_relevances[i]\n",
        "                    doc2 = doc_relevances[j]\n",
        "\n",
        "                    # Gán nhãn theo thuật toán RankNet\n",
        "                    if doc1['relevance'] > doc2['relevance']:\n",
        "                        label = 1.0  # Doc1 tốt hơn Doc2\n",
        "                    elif doc1['relevance'] < doc2['relevance']:\n",
        "                        label = 0.0  # Doc1 kém hơn Doc2\n",
        "                    else:\n",
        "                        label = 0.5  # Doc1 và Doc2 tương đương (cùng relevance)\n",
        "\n",
        "                    pairwise_sample = {\n",
        "                        \"query\": processed_query,\n",
        "                        \"doc1_id\": doc1['document']['id'],\n",
        "                        \"doc1_text\": doc_texts[doc1['doc_idx']],\n",
        "                        \"doc1_relevance\": doc1['relevance'],\n",
        "                        \"doc1_similarity\": float(doc1['similarity']),\n",
        "                        \"doc2_id\": doc2['document']['id'],\n",
        "                        \"doc2_text\": doc_texts[doc2['doc_idx']],\n",
        "                        \"doc2_relevance\": doc2['relevance'],\n",
        "                        \"doc2_similarity\": float(doc2['similarity']),\n",
        "                        \"label\": label  # 1.0, 0.5, hoặc 0.0\n",
        "                    }\n",
        "                    pairwise_data.append(pairwise_sample)\n",
        "        return pairwise_data"
      ],
      "metadata": {
        "id": "8Q-8K8nHKJvX"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuẩn bị dữ liệu, lấy ra so sánh\n",
        "class RankNetDataset(Dataset):\n",
        "    \"\"\"Dataset cho RankNet training\"\"\"\n",
        "    def __init__(self, pairwise_data, sentence_model):\n",
        "        self.data = pairwise_data\n",
        "        self.sentence_model = sentence_model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    #  Trả về một dictionary chứa feature1, feature2, và label\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Encode query và documents\n",
        "        query_emb = self.sentence_model.encode([sample['query']])[0]\n",
        "        doc1_emb = self.sentence_model.encode([sample['doc1_text']])[0]\n",
        "        doc2_emb = self.sentence_model.encode([sample['doc2_text']])[0]\n",
        "\n",
        "        # Tạo features bằng cách concat query+doc\n",
        "        feature1 = np.concatenate([query_emb, doc1_emb])\n",
        "        feature2 = np.concatenate([query_emb, doc2_emb])\n",
        "\n",
        "        return {\n",
        "            'feature1': torch.tensor(feature1, dtype=torch.float32),\n",
        "            'feature2': torch.tensor(feature2, dtype=torch.float32),\n",
        "            'label': torch.tensor(sample['label'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_ranking(model, documents, queries, sentence_model, device, top_k=10):\n",
        "    \"\"\"\n",
        "    Đánh giá mô hình bằng cách rank lại documents cho mỗi query\n",
        "    và tính pairwise accuracy\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    retriever = DocumentRetriever(documents, sentence_model, \"vietnamese-stopwords.txt\")\n",
        "    results = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for query in queries:\n",
        "            # Preprocess query\n",
        "            processed_query_tokens = retriever.preprocess(query)\n",
        "            processed_query = \" \".join(processed_query_tokens)\n",
        "\n",
        "            # Encode query\n",
        "            query_emb = sentence_model.encode([processed_query])[0]\n",
        "\n",
        "            # Score tất cả documents\n",
        "            doc_scores = []\n",
        "            for i, doc in enumerate(documents):\n",
        "                # Preprocess document\n",
        "                processed_doc_tokens = retriever.preprocess(doc['value'])\n",
        "                processed_doc = \" \".join(processed_doc_tokens)\n",
        "\n",
        "                # Encode document\n",
        "                doc_emb = sentence_model.encode([processed_doc])[0]\n",
        "\n",
        "                # Tạo feature vector\n",
        "                feature = torch.tensor(np.concatenate([query_emb, doc_emb]),\n",
        "                                     dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "                # Tính score\n",
        "                score = model(feature).item()\n",
        "                doc_scores.append((i, score, doc))\n",
        "\n",
        "            # Sắp xếp theo score giảm dần\n",
        "            doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Lấy top k\n",
        "            top_docs = doc_scores[:top_k]\n",
        "\n",
        "            results[query] = {\n",
        "                'top_documents': [(doc['id'], doc['value'], score) for _, score, doc in top_docs],\n",
        "                'all_scores': doc_scores\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QpI7PnRkybGn"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hàm huấn luyện\n",
        "def train_ranknet(model, dataloader, optimizer, device, num_epochs=10):\n",
        "    \"\"\"Huấn luyện mô hình RankNet\"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        for batch in progress_bar:\n",
        "            feature1 = batch['feature1'].to(device)\n",
        "            feature2 = batch['feature2'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            score1 = model(feature1)\n",
        "            score2 = model(feature2)\n",
        "\n",
        "            # Tính loss\n",
        "            loss = ranknet_loss(score1.squeeze(), score2.squeeze(), labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({'Loss': f\"{loss.item():.4f}\"})\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KcoSWYiDK4IR"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm đánh giá kết quả\n",
        "def calculate_pairwise_accuracy(model, test_data, sentence_model, device):\n",
        "    \"\"\"Tính Pairwise Accuracy\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in tqdm(test_data, desc=\"Tính Pairwise Accuracy\"):\n",
        "            # Encode features\n",
        "            query_emb = sentence_model.encode([sample['query']])[0]\n",
        "            doc1_emb = sentence_model.encode([sample['doc1_text']])[0]\n",
        "            doc2_emb = sentence_model.encode([sample['doc2_text']])[0]\n",
        "\n",
        "            feature1 = torch.tensor(np.concatenate([query_emb, doc1_emb]),\n",
        "                                  dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            feature2 = torch.tensor(np.concatenate([query_emb, doc2_emb]),\n",
        "                                  dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            # Tính scores\n",
        "            score1 = model(feature1).item()\n",
        "            score2 = model(feature2).item()\n",
        "\n",
        "            # Dự đoán\n",
        "            if sample['label'] == 1.0:  # doc1 should be ranked higher\n",
        "                if score1 > score2:\n",
        "                    correct_predictions += 1\n",
        "            elif sample['label'] == 0.0:  # doc2 should be ranked higher\n",
        "                if score2 > score1:\n",
        "                    correct_predictions += 1\n",
        "            else:  # Equal relevance (label = 0.5)\n",
        "                # Coi như đúng nếu difference nhỏ\n",
        "                if abs(score1 - score2) < 0.1:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "            total_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "CFhp1xa9MT7Y"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chạy chương trình** Tính toán chính"
      ],
      "metadata": {
        "id": "nX9zBcDTydNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Thiết lập device\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Sử dụng device: {device}\")\n",
        "\n",
        "        # Bước 1: Tải tài liệu và khởi tạo sentence-transformer\n",
        "        documents_path = \"/content/cadao_tucngu_50_1.json\"\n",
        "        # documents_path = \"FetchData/train/anh_em_mot_nha/data.json\"\n",
        "\n",
        "        print(\"Đang tải dữ liệu...\")\n",
        "        with open(documents_path, 'r', encoding='utf-8') as f:\n",
        "            documents = json.load(f)\n",
        "        print(f\"Số tài liệu: {len(documents)}\")\n",
        "\n",
        "        # Khởi tạo sentence transformer\n",
        "        print(\"Đang khởi tạo sentence-transformer...\")\n",
        "        sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') # 384 chiều\n",
        "\n",
        "        # Khởi tạo DocumentRetriever với stopwords\n",
        "        stopword_path = \"/content/vietnamese-stopwords.txt\"\n",
        "        retriever = DocumentRetriever(documents, sentence_model, stopword_path)\n",
        "\n",
        "        # Tạo cặp dữ liệu huấn luyện - Cải thiện với multiple queries\n",
        "        queries = [\n",
        "            \"Tình cảm anh em\",\n",
        "            \"tấm gương hiếu thảo\"\n",
        "        ]\n",
        "\n",
        "        print(\"Đang tạo dữ liệu pairwise training...\")\n",
        "        pairwise_data = retriever.generate_pairwise_training_data(queries, documents)\n",
        "        print(f\"Số cặp dữ liệu training: {len(pairwise_data)}\")\n",
        "\n",
        "        # Lưu dữ liệu pairwise vào file để tái sử dụng\n",
        "        with open(\"/content/input.json\", 'w', encoding='utf-8') as f:\n",
        "            json.dump(pairwise_data, f, ensure_ascii=False, indent=2)\n",
        "        print(\"Đã lưu dữ liệu pairwise vào input.json\")\n",
        "\n",
        "        # Tạo dataset và dataloader\n",
        "        print(\"Đang chuẩn bị dataset...\")\n",
        "        dataset = RankNetDataset(pairwise_data, sentence_model)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "\n",
        "        # Bước 2: Khởi tạo và huấn luyện mô hình RankNet\n",
        "        print(\"Đang khởi tạo mô hình RankNet...\")\n",
        "\n",
        "        # Tính kích thước input (query embedding + document embedding)\n",
        "        sample_query_emb = sentence_model.encode([\"test\"])[0]\n",
        "        input_size = len(sample_query_emb) * 2  # query + document embeddings\n",
        "        print(f\"Input size: {input_size}\")\n",
        "\n",
        "        model = RankNet(input_size=input_size, hidden_size1=256, hidden_size2=128, dropout=0.2)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "        print(\"Bắt đầu huấn luyện mô hình...\")\n",
        "        model = train_ranknet(model, dataloader, optimizer, device, num_epochs=20)\n",
        "\n",
        "        # Lưu mô hình\n",
        "        torch.save(model.state_dict(), 'ranknet_model.pth')\n",
        "        print(\"Đã lưu mô hình vào ranknet_model.pth\")\n",
        "\n",
        "                # Bước 3: Đánh giá độ chính xác của mô hình\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ĐÁNH GIÁ MÔ HÌNH\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Test queries\n",
        "        test_queries = [\n",
        "            \"tình cảm anh em\",\n",
        "            \"tình yêu thương ba mẹ\",\n",
        "        ]\n",
        "\n",
        "        print(\"Đang đánh giá ranking...\")\n",
        "        ranking_results = evaluate_ranking(model, documents, test_queries, sentence_model, device, top_k=10)\n",
        "\n",
        "        # Hiển thị kết quả top 10 cho mỗi query\n",
        "        for query, result in ranking_results.items():\n",
        "            print(f\"\\nQuery: '{query}'\")\n",
        "            print(\"Top 10 documents:\")\n",
        "            for i, (doc_id, doc_text, score) in enumerate(result['top_documents'], 1):\n",
        "                print(f\"{i:2d}. [ID: {doc_id}] Score: {score:.4f}\")\n",
        "                print(f\"    Text: {doc_text[:100]}...\")\n",
        "                print()\n",
        "\n",
        "        # Tính Pairwise Accuracy trên tập test\n",
        "        print(\"Đang tính Pairwise Accuracy...\")\n",
        "\n",
        "        # Tạo test data từ test queries\n",
        "        test_pairwise_data = retriever.generate_pairwise_training_data(test_queries, documents[:50])  # Giới hạn để tính nhanh\n",
        "        accuracy = calculate_pairwise_accuracy(model, test_pairwise_data, sentence_model, device)\n",
        "\n",
        "        print(f\"\\nPairwise Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"HOÀN THÀNH!\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "id": "LpgjMrNrMpLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0e50dfc6-9684-426a-8a28-626625b05dfc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sử dụng device: cuda\n",
            "Đang tải dữ liệu...\n",
            "Số tài liệu: 49\n",
            "Đang khởi tạo sentence-transformer...\n",
            "Đang tạo dữ liệu pairwise training...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Tạo dữ liệu pairwise: 100%|██████████| 2/2 [00:00<00:00, 15.04it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Số cặp dữ liệu training: 2352\n",
            "Đã lưu dữ liệu pairwise vào input.json\n",
            "Đang chuẩn bị dataset...\n",
            "Đang khởi tạo mô hình RankNet...\n",
            "Input size: 768\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bắt đầu huấn luyện mô hình...\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1/20: 100%|██████████| 74/74 [01:11<00:00,  1.03it/s, Loss=0.4176]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20, Average Loss: 0.5845\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2/20: 100%|██████████| 74/74 [01:12<00:00,  1.02it/s, Loss=0.3751]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2/20, Average Loss: 0.4841\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3/20: 100%|██████████| 74/74 [01:11<00:00,  1.03it/s, Loss=0.4990]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3/20, Average Loss: 0.4627\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4/20: 100%|██████████| 74/74 [01:12<00:00,  1.02it/s, Loss=0.4204]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4/20, Average Loss: 0.4510\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5/20: 100%|██████████| 74/74 [01:11<00:00,  1.04it/s, Loss=0.2884]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5/20, Average Loss: 0.4487\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 6/20: 100%|██████████| 74/74 [01:11<00:00,  1.03it/s, Loss=0.4401]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 6/20, Average Loss: 0.4478\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 74/74 [01:12<00:00,  1.02it/s, Loss=0.5418]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Average Loss: 0.4438\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 74/74 [01:11<00:00,  1.04it/s, Loss=0.5794]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Average Loss: 0.4415\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20: 100%|██████████| 74/74 [01:11<00:00,  1.03it/s, Loss=0.4585]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/20, Average Loss: 0.4431\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/20: 100%|██████████| 74/74 [01:11<00:00,  1.04it/s, Loss=0.3515]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/20, Average Loss: 0.4357\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/20: 100%|██████████| 74/74 [01:11<00:00,  1.04it/s, Loss=0.4761]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 11/20, Average Loss: 0.4354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/20: 100%|██████████| 74/74 [01:11<00:00,  1.03it/s, Loss=0.5364]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 12/20, Average Loss: 0.4323\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/20: 100%|██████████| 74/74 [01:11<00:00,  1.04it/s, Loss=0.5932]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 13/20, Average Loss: 0.4328\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/20: 100%|██████████| 74/74 [01:11<00:00,  1.03it/s, Loss=0.2915]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 14/20, Average Loss: 0.4360\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/20: 100%|██████████| 74/74 [01:10<00:00,  1.05it/s, Loss=0.4388]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 15/20, Average Loss: 0.4336\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/20: 100%|██████████| 74/74 [01:10<00:00,  1.05it/s, Loss=0.3738]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 16/20, Average Loss: 0.4313\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/20: 100%|██████████| 74/74 [01:11<00:00,  1.03it/s, Loss=0.3913]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 17/20, Average Loss: 0.4274\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/20: 100%|██████████| 74/74 [01:10<00:00,  1.04it/s, Loss=0.4456]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 18/20, Average Loss: 0.4348\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/20: 100%|██████████| 74/74 [01:12<00:00,  1.03it/s, Loss=0.5227]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 19/20, Average Loss: 0.4321\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/20: 100%|██████████| 74/74 [01:10<00:00,  1.05it/s, Loss=0.4862]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 20/20, Average Loss: 0.4298\n",
            "Đã lưu mô hình vào ranknet_model.pth\n",
            "\n",
            "==================================================\n",
            "ĐÁNH GIÁ MÔ HÌNH\n",
            "==================================================\n",
            "Đang đánh giá ranking...\n",
            "\n",
            "Query: 'tình cảm anh em'\n",
            "Top 10 documents:\n",
            " 1. [ID: 112] Score: 9.6028\n",
            "    Text: Bầu ơi thương lấy bí cùng, Tuy rằng khác giống nhưng chung một dàn...\n",
            "\n",
            " 2. [ID: 60] Score: 9.5660\n",
            "    Text: Anh em như chân với tay Rách lành đùm bọc, dở hay đỡ đần....\n",
            "\n",
            " 3. [ID: 59] Score: 8.9079\n",
            "    Text: Anh em hiền thật là hiền Chỉ một đồng tiền làm mất lòng nhau...\n",
            "\n",
            " 4. [ID: 647] Score: 8.8157\n",
            "    Text: Nhất cận thị, nhì cận lân...\n",
            "\n",
            " 5. [ID: 33] Score: 7.9753\n",
            "    Text: Anh em như thể tay chân....\n",
            "\n",
            " 6. [ID: 5] Score: 7.7831\n",
            "    Text: Anh em nào phải người xa, Cùng chung bác mẹ một nhà cùng thân....\n",
            "\n",
            " 7. [ID: 75] Score: 7.3975\n",
            "    Text: Anh em như thể chân tay Cùng cha cùng mẹ việc nhà hăng say...\n",
            "\n",
            " 8. [ID: 944] Score: 4.6924\n",
            "    Text: Giàu sang không bằng bạn hiền, nghèo hèn không bằng anh em một nhà...\n",
            "\n",
            " 9. [ID: 952] Score: 4.6924\n",
            "    Text: Giàu sang không bằng bạn hiền, nghèo hèn không bằng anh em một nhà...\n",
            "\n",
            "10. [ID: 43] Score: 4.3103\n",
            "    Text: Anh em trong nhà, đóng cửa dạy nhau...\n",
            "\n",
            "\n",
            "Query: 'tình yêu thương ba mẹ'\n",
            "Top 10 documents:\n",
            " 1. [ID: 112] Score: 9.6188\n",
            "    Text: Bầu ơi thương lấy bí cùng, Tuy rằng khác giống nhưng chung một dàn...\n",
            "\n",
            " 2. [ID: 60] Score: 9.5677\n",
            "    Text: Anh em như chân với tay Rách lành đùm bọc, dở hay đỡ đần....\n",
            "\n",
            " 3. [ID: 647] Score: 8.9192\n",
            "    Text: Nhất cận thị, nhì cận lân...\n",
            "\n",
            " 4. [ID: 59] Score: 8.8319\n",
            "    Text: Anh em hiền thật là hiền Chỉ một đồng tiền làm mất lòng nhau...\n",
            "\n",
            " 5. [ID: 33] Score: 8.1294\n",
            "    Text: Anh em như thể tay chân....\n",
            "\n",
            " 6. [ID: 5] Score: 7.7433\n",
            "    Text: Anh em nào phải người xa, Cùng chung bác mẹ một nhà cùng thân....\n",
            "\n",
            " 7. [ID: 75] Score: 7.2231\n",
            "    Text: Anh em như thể chân tay Cùng cha cùng mẹ việc nhà hăng say...\n",
            "\n",
            " 8. [ID: 944] Score: 4.7287\n",
            "    Text: Giàu sang không bằng bạn hiền, nghèo hèn không bằng anh em một nhà...\n",
            "\n",
            " 9. [ID: 952] Score: 4.7287\n",
            "    Text: Giàu sang không bằng bạn hiền, nghèo hèn không bằng anh em một nhà...\n",
            "\n",
            "10. [ID: 43] Score: 4.3120\n",
            "    Text: Anh em trong nhà, đóng cửa dạy nhau...\n",
            "\n",
            "Đang tính Pairwise Accuracy...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Tạo dữ liệu pairwise: 100%|██████████| 2/2 [00:00<00:00, 16.12it/s]\n",
            "Tính Pairwise Accuracy: 100%|██████████| 2352/2352 [01:13<00:00, 31.88it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pairwise Accuracy: 0.8852 (88.52%)\n",
            "\n",
            "==================================================\n",
            "HOÀN THÀNH!\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}