# -*- coding: utf-8 -*-
"""L2R-RankNet

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16ycMO28lZ7Wx8gd094Ypawh8QZ9xMrfK
"""

!nvidia-smi -L

"""Import c√°c th∆∞ vi·ªán c·∫ßn s·ª≠ d·ª•ng"""

!pip install pyvi
# !pip install sentence_transformers

import json
import math
from typing import List, Dict, Tuple, Optional
import re
from collections import Counter, defaultdict
import os
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np
from itertools import combinations
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
from torch.utils.data import Dataset, DataLoader
from sentence_transformers import SentenceTransformer
from pyvi import ViTokenizer
from tqdm import tqdm

"""ƒê·ªãnh nghƒ©a class RankNet - k·∫øt th·ª´a t·ª´ th∆∞ vi·ªán: torch.nn.Module"""

class RankNet(nn.Module):
    def __init__(self, input_size, hidden_size1=256, hidden_size2=128, dropout=0.2):
        """
        RankNet v·ªõi 2 t·∫ßng ·∫©n

        Args:
            input_size (int): S·ªë features ƒë·∫ßu v√†o
            hidden_size1 (int): S·ªë neurons t·∫ßng ·∫©n th·ª© nh·∫•t
            hidden_size2 (int): S·ªë neurons t·∫ßng ·∫©n th·ª© hai
            dropout (float): T·ª∑ l·ªá dropout ƒë·ªÉ tr√°nh overfitting
        """
        super(RankNet, self).__init__()

        # ƒê·ªãnh nghƒ©a c√°c t·∫ßng
        self.fc1 = nn.Linear(input_size, hidden_size1)
        self.fc2 = nn.Linear(hidden_size1, hidden_size2)
        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer cho score

        # Dropout layers
        self.dropout1 = nn.Dropout(dropout)
        self.dropout2 = nn.Dropout(dropout)

        # Layer normalization thay v√¨ batch normalization ƒë·ªÉ tr√°nh l·ªói khi batch_size=1
        self.ln1 = nn.LayerNorm(hidden_size1)
        self.ln2 = nn.LayerNorm(hidden_size2)

    def forward(self, x):
        """
        Forward pass

        Args:
            x (torch.Tensor): Input tensor c√≥ shape (batch_size, input_size)

        Returns:
            torch.Tensor: Output scores c√≥ shape (batch_size, 1)
        """
        # ƒê·∫£m b·∫£o input l√† tensor v√† c√≥ ƒë√∫ng ki·ªÉu d·ªØ li·ªáu
        if not isinstance(x, torch.Tensor):
            x = torch.tensor(x, dtype=torch.float32)

        # T·∫ßng ·∫©n th·ª© nh·∫•t
        x = self.fc1(x)
        x = self.ln1(x)
        x = F.relu(x)
        x = self.dropout1(x)

        # T·∫ßng ·∫©n th·ª© hai
        x = self.fc2(x)
        x = self.ln2(x)
        x = F.relu(x)
        x = self.dropout2(x)

        # Output layer
        x = self.fc3(x)

        return x

    def predict_rank(self, x1, x2):
        """
        So s√°nh ranking gi·ªØa hai samples

        Args:
            x1, x2 (torch.Tensor): Hai samples c·∫ßn so s√°nh

        Returns:
            torch.Tensor: X√°c su·∫•t x1 ƒë∆∞·ª£c rank cao h∆°n x2
        """
        score1 = self.forward(x1)
        score2 = self.forward(x2)

        # S·ª≠ d·ª•ng sigmoid ƒë·ªÉ chuy·ªÉn v·ªÅ x√°c su·∫•t
        prob = torch.sigmoid(score1 - score2)
        return prob

"""H√†m m·∫•t m√°t"""

def ranknet_loss(s_i, s_j, P_ij):
    diff = s_i - s_j
    P_hat = torch.sigmoid(diff)  # X√°c su·∫•t d·ª± ƒëo√°n PÃÇ·µ¢‚±º
    # Th√™m epsilon ƒë·ªÉ tr√°nh log(0)
    epsilon = 1e-10
    loss = -P_ij * torch.log(P_hat + epsilon) - (1 - P_ij) * torch.log(1 - P_hat + epsilon)
    return loss.mean()

"""M·ªôt s·ªë l·ªõp, h√†m ph·ª• tr·ª£"""

class DataPreparator:
    """
    Class chuy√™n d·ª•ng cho vi·ªác chu·∫©n b·ªã d·ªØ li·ªáu pairwise training
    """

    def __init__(self, sentence_model: SentenceTransformer, stopwords_path: Optional[str] = None):
        """
        Kh·ªüi t·∫°o DataPreparator

        Args:
            sentence_model: M√¥ h√¨nh sentence transformer
            stopwords_path: ƒê∆∞·ªùng d·∫´n file stopwords (optional)
        """
        self.sentence_model = sentence_model
        self.stopwords = set()

        # Load stopwords n·∫øu c√≥
        if stopwords_path and os.path.exists(stopwords_path):
            with open(stopwords_path, 'r', encoding='utf-8') as f:
                self.stopwords = set(line.strip() for line in f if line.strip())
            print(f"ƒê√£ load {len(self.stopwords)} stopwords t·ª´ {stopwords_path}")

    def preprocess_text(self, text: str) -> str:
        """
        Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n v·ªõi PyVi

        Args:
            text: VƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω

        Returns:
            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω
        """
        # 1. Chu·∫©n h√≥a c∆° b·∫£n
        text = text.lower()
        text = re.sub(r'[^\w\s]', ' ', text)  # lo·∫°i b·ªè d·∫•u c√¢u
        text = re.sub(r'\d+', ' ', text)      # lo·∫°i b·ªè s·ªë
        text = re.sub(r'\s+', ' ', text).strip()  # b·ªè kho·∫£ng tr·∫Øng th·ª´a

        # 2. Tokenize v·ªõi PyVi
        tokenized = ViTokenizer.tokenize(text)

        # 3. T√°ch th√†nh danh s√°ch t·ª´ v√† l·ªçc stopwords
        tokens = tokenized.split()
        if self.stopwords:
            tokens = [token for token in tokens if token not in self.stopwords]

        return " ".join(tokens)


    def calculate_relevance_scores(self, query: str, documents: List[Dict],
                                relevance_method: str = 'cosine_ranking') -> List[Tuple[int, float, int]]:
        """
        T√≠nh ƒëi·ªÉm relevance cho t·∫•t c·∫£ documents v·ªõi m·ªôt query
        """
        # Preprocess query
        processed_query = self.preprocess_text(query)

        # Encode query
        query_embedding = self.sentence_model.encode([processed_query])

        # Preprocess v√† encode t·∫•t c·∫£ documents
        doc_texts = []
        for doc in documents:
            processed_doc = self.preprocess_text(doc['value'])
            doc_texts.append(processed_doc)

        doc_embeddings = self.sentence_model.encode(doc_texts)

        # T√≠nh cosine similarity
        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]

        # G√°n relevance labels - S·ª¨A CH√çNH T·∫†I ƒê√ÇY
        doc_relevances = []

        if relevance_method == 'cosine_ranking':
            # S·∫Øp x·∫øp theo similarity v√† g√°n labels theo ranking
            doc_similarities = [(i, sim) for i, sim in enumerate(similarities)]
            doc_similarities.sort(key=lambda x: x[1], reverse=True)

            # Ph√¢n ph·ªëi relevance t·ªët h∆°n
            total_docs = len(doc_similarities)
            for rank, (doc_idx, similarity) in enumerate(doc_similarities):
                if rank < max(1, total_docs // 10):  # Top 10%: Highly relevant
                    relevance = 3
                elif rank < max(2, total_docs // 5):  # Top 20%: Relevant
                    relevance = 2
                elif rank < max(3, total_docs // 3):  # Top 33%: Somewhat relevant
                    relevance = 1
                else:  # C√≤n l·∫°i: Not relevant
                    relevance = 0

                doc_relevances.append((doc_idx, float(similarity), relevance))

        elif relevance_method == 'threshold_based':
            # ƒêi·ªÅu ch·ªânh threshold cho ph√π h·ª£p h∆°n
            for doc_idx, similarity in enumerate(similarities):
                if similarity >= 0.7:  # Gi·∫£m t·ª´ 0.8 xu·ªëng 0.7
                    relevance = 3
                elif similarity >= 0.5:  # Gi·∫£m t·ª´ 0.6 xu·ªëng 0.5
                    relevance = 2
                elif similarity >= 0.3:  # Gi·∫£m t·ª´ 0.4 xu·ªëng 0.3
                    relevance = 1
                else:
                    relevance = 0

                doc_relevances.append((doc_idx, float(similarity), relevance))

        return doc_relevances

    def generate_pairwise_data(self, queries: List[str], documents: List[Dict],
                             relevance_method: str = 'cosine_ranking',
                             max_pairs_per_query: Optional[int] = None,
                             balance_labels: bool = True) -> List[Dict]:
        """
        T·∫°o d·ªØ li·ªáu pairwise training

        Args:
            queries: Danh s√°ch queries
            documents: Danh s√°ch documents
            relevance_method: Ph∆∞∆°ng ph√°p t√≠nh relevance
            max_pairs_per_query: Gi·ªõi h·∫°n s·ªë c·∫∑p per query (optional)
            balance_labels: C√≥ c√¢n b·∫±ng labels kh√¥ng

        Returns:
            List[Dict]: Danh s√°ch pairwise samples
        """
        print(f"B·∫Øt ƒë·∫ßu t·∫°o d·ªØ li·ªáu pairwise v·ªõi {len(queries)} queries v√† {len(documents)} documents")

        all_pairwise_data = []
        label_counts = {0.0: 0, 0.5: 0, 1.0: 0}

        for query_idx, query in enumerate(tqdm(queries, desc="T·∫°o d·ªØ li·ªáu pairwise")):
            # T√≠nh relevance scores
            doc_relevances = self.calculate_relevance_scores(query, documents, relevance_method)

            # T·∫°o t·∫•t c·∫£ c√°c c·∫∑p possible
            query_pairs = []

            for i in range(len(doc_relevances)):
                for j in range(i + 1, len(doc_relevances)):
                    doc1_idx, doc1_sim, doc1_rel = doc_relevances[i]
                    doc2_idx, doc2_sim, doc2_rel = doc_relevances[j]

                    # X√°c ƒë·ªãnh label cho c·∫∑p
                    if doc1_rel > doc2_rel:
                        label = 1.0  # Doc1 t·ªët h∆°n Doc2
                    elif doc1_rel < doc2_rel:
                        label = 0.0  # Doc1 k√©m h∆°n Doc2
                    else:
                        label = 0.5  # B·∫±ng nhau

                    # T·∫°o sample
                    pairwise_sample = {
                        "query_id": query_idx,
                        "query": query,
                        "query_processed": self.preprocess_text(query),
                        "doc1_id": documents[doc1_idx]['id'],
                        "doc1_text": documents[doc1_idx]['value'],
                        "doc1_processed": self.preprocess_text(documents[doc1_idx]['value']),
                        "doc1_relevance": doc1_rel,
                        "doc1_similarity": doc1_sim,
                        "doc2_id": documents[doc2_idx]['id'],
                        "doc2_text": documents[doc2_idx]['value'],
                        "doc2_processed": self.preprocess_text(documents[doc2_idx]['value']),
                        "doc2_relevance": doc2_rel,
                        "doc2_similarity": doc2_sim,
                        "label": label,
                        "relevance_method": relevance_method
                    }

                    query_pairs.append(pairwise_sample)
                    label_counts[label] += 1

            # Gi·ªõi h·∫°n s·ªë c·∫∑p per query n·∫øu c·∫ßn
            if max_pairs_per_query and len(query_pairs) > max_pairs_per_query:
                # Sampling c√≥ c√¢n b·∫±ng labels
                if balance_labels:
                    query_pairs = self._balanced_sampling(query_pairs, max_pairs_per_query)
                else:
                    query_pairs = query_pairs[:max_pairs_per_query]

            all_pairwise_data.extend(query_pairs)

        # In th·ªëng k√™
        total_pairs = len(all_pairwise_data)
        print(f"\nTh·ªëng k√™ d·ªØ li·ªáu pairwise:")
        print(f"T·ªïng s·ªë c·∫∑p: {total_pairs}")
        print(f"Label 0.0 (doc1 < doc2): {label_counts[0.0]} ({label_counts[0.0]/total_pairs*100:.1f}%)")
        print(f"Label 0.5 (doc1 = doc2): {label_counts[0.5]} ({label_counts[0.5]/total_pairs*100:.1f}%)")
        print(f"Label 1.0 (doc1 > doc2): {label_counts[1.0]} ({label_counts[1.0]/total_pairs*100:.1f}%)")

        return all_pairwise_data

    def _balanced_sampling(self, pairs: List[Dict], max_pairs: int) -> List[Dict]:
        """
        L·∫•y m·∫´u c√¢n b·∫±ng theo labels

        Args:
            pairs: Danh s√°ch t·∫•t c·∫£ c√°c c·∫∑p
            max_pairs: S·ªë c·∫∑p t·ªëi ƒëa

        Returns:
            List[Dict]: Danh s√°ch c·∫∑p ƒë√£ ƒë∆∞·ª£c sampling
        """
        # Ph√¢n lo·∫°i theo labels
        label_groups = {0.0: [], 0.5: [], 1.0: []}
        for pair in pairs:
            label_groups[pair['label']].append(pair)

        # T√≠nh s·ªë m·∫´u cho m·ªói label (c√¢n b·∫±ng)
        samples_per_label = max_pairs // 3
        remaining = max_pairs % 3

        sampled_pairs = []

        # L·∫•y m·∫´u t·ª´ng label
        for i, (label, group) in enumerate(label_groups.items()):
            n_samples = samples_per_label + (1 if i < remaining else 0)
            n_samples = min(n_samples, len(group))  # Kh√¥ng v∆∞·ª£t qu√° s·ªë c√≥ s·∫µn

            # Random sampling
            import random
            sampled = random.sample(group, n_samples)
            sampled_pairs.extend(sampled)

        return sampled_pairs

    def save_pairwise_data(self, pairwise_data: List[Dict], file_path: str,
                          metadata: Optional[Dict] = None):
        """
        L∆∞u d·ªØ li·ªáu pairwise v√†o file

        Args:
            pairwise_data: D·ªØ li·ªáu pairwise
            file_path: ƒê∆∞·ªùng d·∫´n file
            metadata: Metadata b·ªï sung (optional)
        """
        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥
        os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else '.', exist_ok=True)

        # Chu·∫©n b·ªã data ƒë·ªÉ l∆∞u
        data_to_save = {
            "metadata": {
                "total_pairs": len(pairwise_data),
                "created_by": "DataPreparator",
                "format_version": "1.0",
                **(metadata or {})
            },
            "pairwise_data": pairwise_data
        }

        # L∆∞u file
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(data_to_save, f, ensure_ascii=False, indent=2)

        print(f"ƒê√£ l∆∞u {len(pairwise_data)} c·∫∑p d·ªØ li·ªáu v√†o {file_path}")

    def load_pairwise_data(self, file_path: str) -> Tuple[List[Dict], Dict]:
        """
        Load d·ªØ li·ªáu pairwise t·ª´ file

        Args:
            file_path: ƒê∆∞·ªùng d·∫´n file

        Returns:
            Tuple[List[Dict], Dict]: (pairwise_data, metadata)
        """
        if not os.path.exists(file_path):
            raise FileNotFoundError(f"File kh√¥ng t·ªìn t·∫°i: {file_path}")

        with open(file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        pairwise_data = data.get('pairwise_data', [])
        metadata = data.get('metadata', {})

        print(f"ƒê√£ load {len(pairwise_data)} c·∫∑p d·ªØ li·ªáu t·ª´ {file_path}")

        return pairwise_data, metadata
def prepare_training_data(documents_path: str, queries: List[str],
                         output_file: str = "document_pairs.json",
                         sentence_model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
                         stopwords_path: Optional[str] = None,
                         relevance_method: str = 'cosine_ranking',
                         max_pairs_per_query: Optional[int] = None):
    """
    H√†m wrapper ƒë·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu training m·ªôt c√°ch d·ªÖ d√†ng

    Args:
        documents_path: ƒê∆∞·ªùng d·∫´n file documents JSON
        queries: Danh s√°ch queries
        output_file: T√™n file output
        sentence_model_name: T√™n sentence transformer model
        stopwords_path: ƒê∆∞·ªùng d·∫´n stopwords file
        relevance_method: Ph∆∞∆°ng ph√°p t√≠nh relevance
        max_pairs_per_query: Gi·ªõi h·∫°n c·∫∑p per query

    Returns:
        List[Dict]: D·ªØ li·ªáu pairwise ƒë√£ t·∫°o
    """
    print("="*60)
    print("CHU·∫®N B·ªä D·ªÆ LI·ªÜU PAIRWISE CHO RANKNET")
    print("="*60)

    # 1. Load documents
    print(f"1. ƒêang load documents t·ª´ {documents_path}...")
    with open(documents_path, 'r', encoding='utf-8') as f:
        documents = json.load(f)
    print(f"   ‚úì ƒê√£ load {len(documents)} documents")

    # 2. Kh·ªüi t·∫°o sentence transformer
    print(f"2. ƒêang kh·ªüi t·∫°o sentence transformer: {sentence_model_name}...")
    sentence_model = SentenceTransformer(sentence_model_name)
    print(f"   ‚úì Model ƒë√£ s·∫µn s√†ng")

    # 3. Kh·ªüi t·∫°o DataPreparator
    print("3. ƒêang kh·ªüi t·∫°o DataPreparator...")
    preparator = DataPreparator(sentence_model, stopwords_path)
    print(f"   ‚úì DataPreparator ƒë√£ s·∫µn s√†ng")

    # 4. T·∫°o pairwise data
    print("4. ƒêang t·∫°o d·ªØ li·ªáu pairwise...")
    pairwise_data = preparator.generate_pairwise_data(
        queries=queries,
        documents=documents,
        relevance_method=relevance_method,
        max_pairs_per_query=max_pairs_per_query,
        balance_labels=True
    )
    print(f"   ‚úì ƒê√£ t·∫°o {len(pairwise_data)} c·∫∑p d·ªØ li·ªáu")

    # 5. L∆∞u file
    print(f"5. ƒêang l∆∞u v√†o {output_file}...")
    metadata = {
        "documents_path": documents_path,
        "num_queries": len(queries),
        "queries": queries,
        "relevance_method": relevance_method,
        "sentence_model": sentence_model_name,
        "max_pairs_per_query": max_pairs_per_query
    }

    preparator.save_pairwise_data(pairwise_data, output_file, metadata)
    print(f"   ‚úì D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng")

    print("="*60)
    print("HO√ÄN TH√ÄNH CHU·∫®N B·ªä D·ªÆ LI·ªÜU!")
    print("="*60)

    return pairwise_data

# Chu·∫©n b·ªã d·ªØ li·ªáu, l·∫•y ra so s√°nh
class RankNetDataset(Dataset):
    """Dataset cho RankNet training"""
    def __init__(self, pairwise_data, sentence_model):
        self.data = pairwise_data
        self.sentence_model = sentence_model

    def __len__(self):
        return len(self.data)

    #  Tr·∫£ v·ªÅ m·ªôt dictionary ch·ª©a feature1, feature2, v√† label
    def __getitem__(self, idx):
        sample = self.data[idx]

        # Encode query v√† documents
        query_emb = self.sentence_model.encode([sample['query']])[0]
        doc1_emb = self.sentence_model.encode([sample['doc1_text']])[0]
        doc2_emb = self.sentence_model.encode([sample['doc2_text']])[0]

        # T·∫°o features b·∫±ng c√°ch concat query+doc
        feature1 = np.concatenate([query_emb, doc1_emb])
        feature2 = np.concatenate([query_emb, doc2_emb])

        return {
            'feature1': torch.tensor(feature1, dtype=torch.float32),
            'feature2': torch.tensor(feature2, dtype=torch.float32),
            'label': torch.tensor(sample['label'], dtype=torch.float32)
        }


def evaluate_ranking(model, documents, queries, sentence_model, preparator, device, top_k=10):
    """
    ƒê√°nh gi√° m√¥ h√¨nh b·∫±ng c√°ch rank l·∫°i documents cho m·ªói query
    v√† t√≠nh pairwise accuracy
    """
    model.eval()
    model.to(device)

    results = {}

    with torch.no_grad():
        for query in queries:
            # S·ª¨A: G·ªçi ƒë√∫ng method preprocess_text
            processed_query = preparator.preprocess_text(query)

            # Encode query
            query_emb = sentence_model.encode([processed_query])[0]

            # Score t·∫•t c·∫£ documents
            doc_scores = []
            for doc in documents:
                # S·ª¨A: G·ªçi ƒë√∫ng method preprocess_text
                processed_doc = preparator.preprocess_text(doc['value'])

                doc_emb = sentence_model.encode([processed_doc])[0]

                feature = torch.tensor(
                    np.concatenate([query_emb, doc_emb]),
                    dtype=torch.float32
                ).unsqueeze(0).to(device)

                score = model(feature).item()
                doc_scores.append((doc['id'], score, doc))

            # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo score
            doc_scores.sort(key=lambda x: x[1], reverse=True)

            # L·∫•y top k
            top_docs = doc_scores[:top_k]

            results[query] = {
                'top_documents': [(doc_id, doc['value'], score) for doc_id, score, doc in top_docs],
                'all_scores': doc_scores
            }

    return results

# H√†m hu·∫•n luy·ªán
def train_ranknet(model, dataloader, optimizer, device, num_epochs=20,
                          loss_type='cross_entropy', patience=5, min_delta=1e-4):
    """
    Hu·∫•n luy·ªán m√¥ h√¨nh RankNet v·ªõi c√°c c·∫£i ti·∫øn

    Args:
        model: RankNet model
        dataloader: DataLoader
        optimizer: optimizer
        device: cuda/cpu
        num_epochs: s·ªë epochs
        loss_type: lo·∫°i loss function
        patience: s·ªë epochs ch·ªù ƒë·ªÉ early stopping
        min_delta: threshold nh·ªè nh·∫•t ƒë·ªÉ coi l√† improvement
    """
    model.train()
    model.to(device)

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6
    )

    # Early stopping variables
    best_loss = float('inf')
    patience_counter = 0
    loss_history = []

    for epoch in range(num_epochs):
        total_loss = 0
        num_batches = 0

        progress_bar = tqdm(dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")

        for batch in progress_bar:
            feature1 = batch['feature1'].to(device)
            feature2 = batch['feature2'].to(device)
            labels = batch['label'].to(device)

            optimizer.zero_grad()

            # Forward pass
            score1 = model(feature1).squeeze()
            score2 = model(feature2).squeeze()

            # T√≠nh loss v·ªõi h√†m c·∫£i ti·∫øn
            loss = ranknet_loss(score1, score2, labels)

            # Backward pass
            loss.backward()

            # Gradient clipping ƒë·ªÉ tr√°nh exploding gradients
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)

            optimizer.step()

            total_loss += loss.item()
            num_batches += 1

            progress_bar.set_postfix({
                'Loss': f"{loss.item():.4f}",
                'LR': f"{optimizer.param_groups[0]['lr']:.6f}"
            })

        avg_loss = total_loss / num_batches
        loss_history.append(avg_loss)

        # Learning rate scheduling
        scheduler.step(avg_loss)

        print(f"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}")

        # Early stopping check
        if avg_loss < best_loss - min_delta:
            best_loss = avg_loss
            patience_counter = 0
            # Save best model
            torch.save(model.state_dict(), 'best_ranknet_model.pth')
            print(f"New best model saved with loss: {best_loss:.4f}")
        else:
            patience_counter += 1

        if patience_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            print(f"Loading best model with loss: {best_loss:.4f}")
            model.load_state_dict(torch.load('best_ranknet_model.pth'))
            break

    return model

# H√†m ƒë√°nh gi√° k·∫øt qu·∫£
def calculate_pairwise_accuracy(model, test_data, sentence_model, device):
    """T√≠nh Pairwise Accuracy"""
    model.eval()
    model.to(device)

    correct_predictions = 0
    total_predictions = 0

    with torch.no_grad():
        for sample in tqdm(test_data, desc="T√≠nh Pairwise Accuracy"):
            # Encode features
            query_emb = sentence_model.encode([sample['query']])[0]
            doc1_emb = sentence_model.encode([sample['doc1_text']])[0]
            doc2_emb = sentence_model.encode([sample['doc2_text']])[0]

            feature1 = torch.tensor(np.concatenate([query_emb, doc1_emb]),
                                  dtype=torch.float32).unsqueeze(0).to(device)
            feature2 = torch.tensor(np.concatenate([query_emb, doc2_emb]),
                                  dtype=torch.float32).unsqueeze(0).to(device)

            # T√≠nh scores
            score1 = model(feature1).item()
            score2 = model(feature2).item()

            # D·ª± ƒëo√°n
            if sample['label'] == 1.0:  # doc1 should be ranked higher
                if score1 > score2:
                    correct_predictions += 1
            elif sample['label'] == 0.0:  # doc2 should be ranked higher
                if score2 > score1:
                    correct_predictions += 1
            else:  # Equal relevance (label = 0.5)
                # Coi nh∆∞ ƒë√∫ng n·∫øu difference nh·ªè
                if abs(score1 - score2) < 0.1:
                    correct_predictions += 1

            total_predictions += 1

    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0
    return accuracy

  # H√†m test
def test_specific_document_pair(model, sentence_model, preparator, documents,
                              query, doc_id1, doc_id2, device):
  """
  Test m√¥ h√¨nh v·ªõi c·∫∑p t√†i li·ªáu c·ª• th·ªÉ theo ID

  Args:
      model: RankNet model ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán
      sentence_model: SentenceTransformer model
      preparator: DataPreparator
      documents: List t·∫•t c·∫£ documents
      query: Query string ƒë·ªÉ test
      doc_id1, doc_id2: ID c·ªßa 2 documents c·∫ßn so s√°nh
      device: cuda/cpu
  """
  print(f"="*60)
  print(f"TEST C·∫∂P T√ÄI LI·ªÜU C·ª§ TH·ªÇ")
  print(f"="*60)

  # T√¨m documents theo ID
  doc1 = None
  doc2 = None

  for doc in documents:
      if doc['id'] == doc_id1:
          doc1 = doc
      elif doc['id'] == doc_id2:
          doc2 = doc

  if doc1 is None:
      print(f"‚ùå Kh√¥ng t√¨m th·∫•y document v·ªõi ID: {doc_id1}")
      return
  if doc2 is None:
      print(f"‚ùå Kh√¥ng t√¨m th·∫•y document v·ªõi ID: {doc_id2}")
      return

  print(f"üìÑ Query: '{query}'")
  print(f"üìÑ Document 1 [ID: {doc_id1}]: '{doc1['value']}'")
  print(f"üìÑ Document 2 [ID: {doc_id2}]: '{doc2['value']}'")
  print()

  # Preprocess v√† encode
  processed_query = preparator.preprocess_text(query)
  processed_doc1 = preparator.preprocess_text(doc1['value'])
  processed_doc2 = preparator.preprocess_text(doc2['value'])

  query_emb = sentence_model.encode([processed_query])[0]
  doc1_emb = sentence_model.encode([processed_doc1])[0]
  doc2_emb = sentence_model.encode([processed_doc2])[0]

  # T·∫°o features (concatenation)
  feature1 = np.concatenate([query_emb, doc1_emb])
  feature2 = np.concatenate([query_emb, doc2_emb])

  print(f"üîß Preprocessed query: '{processed_query}'")
  print(f"üîß Preprocessed doc1: '{processed_doc1}'")
  print(f"üîß Preprocessed doc2: '{processed_doc2}'")
  print(f"üîß Feature vector size: {len(feature1)}")
  print()

  # Test v·ªõi model
  model.eval()
  with torch.no_grad():
      # Chuy·ªÉn th√†nh tensor
      feature1_tensor = torch.tensor(feature1, dtype=torch.float32).unsqueeze(0).to(device)
      feature2_tensor = torch.tensor(feature2, dtype=torch.float32).unsqueeze(0).to(device)

      # T√≠nh scores
      score1 = model(feature1_tensor).item()
      score2 = model(feature2_tensor).item()

      # T√≠nh x√°c su·∫•t
      diff = score1 - score2
      probability_1_better_2 = torch.sigmoid(torch.tensor(diff)).item()

      print(f"üìä K·∫æT QU·∫¢:")
      print(f"   Score Document 1: {score1:.4f}")
      print(f"   Score Document 2: {score2:.4f}")
      print(f"   Score difference: {diff:.4f}")
      print(f"   P(Doc1 > Doc2): {probability_1_better_2:.4f} ({probability_1_better_2*100:.1f}%)")
      print()

      # K·∫øt lu·∫≠n
      if probability_1_better_2 > 0.6:
          conclusion = f"Document 1 (ID: {doc_id1}) c√≥ kh·∫£ nƒÉng cao h∆°n li√™n quan v·ªõi query"
      elif probability_1_better_2 < 0.4:
          conclusion = f"Document 2 (ID: {doc_id2}) c√≥ kh·∫£ nƒÉng cao h∆°n li√™n quan v·ªõi query"
      else:
          conclusion = "Hai documents c√≥ m·ª©c ƒë·ªô li√™n quan t∆∞∆°ng ƒë∆∞∆°ng"

      print(f"üéØ K·∫æT LU·∫¨N: {conclusion}")

      # T√≠nh cosine similarity ƒë·ªÉ so s√°nh
      from sklearn.metrics.pairwise import cosine_similarity
      query_doc1_sim = cosine_similarity([query_emb], [doc1_emb])[0][0]
      query_doc2_sim = cosine_similarity([query_emb], [doc2_emb])[0][0]

      print(f"\nüìà SO S√ÅNH V·ªöI COSINE SIMILARITY:")
      print(f"   Query vs Doc1: {query_doc1_sim:.4f}")
      print(f"   Query vs Doc2: {query_doc2_sim:.4f}")

      if query_doc1_sim > query_doc2_sim:
          cosine_conclusion = f"Theo cosine similarity: Document 1 li√™n quan h∆°n"
      elif query_doc2_sim > query_doc1_sim:
          cosine_conclusion = f"Theo cosine similarity: Document 2 li√™n quan h∆°n"
      else:
          cosine_conclusion = f"Theo cosine similarity: Hai documents t∆∞∆°ng ƒë∆∞∆°ng"

      print(f"   {cosine_conclusion}")

      # So s√°nh k·∫øt qu·∫£
      model_prefers_doc1 = probability_1_better_2 > 0.5
      cosine_prefers_doc1 = query_doc1_sim > query_doc2_sim

      if model_prefers_doc1 == cosine_prefers_doc1:
          print(f"   ‚úÖ Model v√† Cosine similarity ƒë·ªìng √Ω v·ªõi nhau")
      else:
          print(f"   ‚ö†Ô∏è  Model v√† Cosine similarity c√≥ k·∫øt qu·∫£ kh√°c nhau")

  print(f"="*60)

"""**Ch·∫°y ch∆∞∆°ng tr√¨nh** T√≠nh to√°n ch√≠nh"""

if __name__ == "__main__":
    try:
        # Thi·∫øt l·∫≠p device
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"S·ª≠ d·ª•ng device: {device}")

        # B∆∞·ªõc 1: T·∫£i t√†i li·ªáu v√† kh·ªüi t·∫°o sentence-transformer
        documents_path = "/content/cadao_tucngu_50_1.json"
        pairwise_data_file = "/content/document_pairs.json"
        # documents_path = "FetchData/train/anh_em_mot_nha/data.json"

        # Ch·∫°y document ƒë·ªÉ l·∫•y ƒë√°nh gi√° v·ªõi k·∫øt qu·∫£
        with open(documents_path, 'r', encoding='utf-8') as f:
            documents = json.load(f)
        stopwords_path = "/content/vietnamese-stopwords.txt"
        # T·∫°o c·∫∑p d·ªØ li·ªáu hu·∫•n luy·ªán - C·∫£i thi·ªán v·ªõi multiple queries
        queries = [
            "Anh em nh∆∞ th·ªÉ tay ch√¢n",
            "Anh em thu·∫≠n h√≤a"
        ]

        if os.path.exists(pairwise_data_file):
            print(f"1. Load d·ªØ li·ªáu pairwise t·ª´ {pairwise_data_file}...")

            # Kh·ªüi t·∫°o sentence model tr∆∞·ªõc
            sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
            preparator = DataPreparator(sentence_model)

            # Load d·ªØ li·ªáu
            pairwise_data, metadata = preparator.load_pairwise_data(pairwise_data_file)
            print(f"   ‚úì ƒê√£ load {len(pairwise_data)} c·∫∑p t·ª´ file c√≥ s·∫µn")
            print(f"   ‚úì Metadata: {metadata}")

        else:
            print("1. T·∫°o d·ªØ li·ªáu pairwise m·ªõi...")

            # T·∫°o d·ªØ li·ªáu pairwise
            pairwise_data = prepare_training_data(
                documents_path=documents_path,
                queries=queries,
                output_file=pairwise_data_file,
                stopwords_path=None,
                relevance_method="cosine_ranking",
                max_pairs_per_query=1000
            )

            # Kh·ªüi t·∫°o sentence model
            sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') #384 neurons
        # T·∫°o dataset v√† dataloader
        print("ƒêang chu·∫©n b·ªã dataset...")
        dataset = RankNetDataset(pairwise_data, sentence_model)
        dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)

        # B∆∞·ªõc 2: Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh RankNet
        print("ƒêang kh·ªüi t·∫°o m√¥ h√¨nh RankNet...")

        # T√≠nh k√≠ch th∆∞·ªõc input (query embedding + document embedding)
        sample_query_emb = sentence_model.encode(["test"])[0]
        input_size = len(sample_query_emb) * 2  # query + document embeddings
        print(f"Input size: {input_size}")

        model = RankNet(input_size=input_size, hidden_size1=128, hidden_size2=64, dropout=0.5)
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)

        print("B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh...")
        model = train_ranknet(model, dataloader, optimizer, device, num_epochs=10)

        # L∆∞u m√¥ h√¨nh
        torch.save(model.state_dict(), 'ranknet_model.pth')
        print("ƒê√£ l∆∞u m√¥ h√¨nh v√†o ranknet_model.pth")

                # B∆∞·ªõc 3: ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh
        print("\n" + "="*50)
        print("ƒê√ÅNH GI√Å M√î H√åNH")
        print("="*50)

        # Test queries
        test_queries = [
            "Anh em nh∆∞ th·ªÉ tay ch√¢n",
        ]

        print("ƒêang ƒë√°nh gi√° ranking...")
        ranking_results = evaluate_ranking(model, documents, test_queries, sentence_model, preparator, device, top_k=10)


        # Hi·ªÉn th·ªã k·∫øt qu·∫£ top 10 cho m·ªói query
        for query, result in ranking_results.items():
            print(f"\nQuery: '{query}'")
            print("Top 10 documents:")
            for i, (doc_id, doc_text, score) in enumerate(result['top_documents'], 1):
                print(f"{i:2d}. [ID: {doc_id}] Score: {score:.4f}")
                print(f"    Text: {doc_text[:100]}...")
                print()

        # T√≠nh Pairwise Accuracy tr√™n t·∫≠p test
        print("ƒêang t√≠nh Pairwise Accuracy...")

        # T·∫°o test data t·ª´ test queries
        test_pairwise_data = prepare_training_data(
                documents_path=documents_path,
                queries=queries,
                output_file=pairwise_data_file,
                stopwords_path=None,
                relevance_method="cosine_ranking",
                max_pairs_per_query=1000
            )  # Gi·ªõi h·∫°n ƒë·ªÉ t√≠nh nhanh
        accuracy = calculate_pairwise_accuracy(model, test_pairwise_data, sentence_model, device)

        print(f"\nPairwise Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)")

        print("\n" + "="*50)
        print("HO√ÄN TH√ÄNH!")
        print("="*50)

    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()

"""Ti·∫øn h√†nh ki·ªÉm th·ª≠ m√¥ h√¨nh [RankNet] ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán, ch√∫ng ta s·∫Ω ki·ªÉm tra ƒë∆°n gi·∫£n b·∫±ng c√°ch - ƒë·∫ßu v√†o ch√∫ng ta s·∫Ω cho 2 t√†i li·ªáu ( di ) v√† ( dj ) (·ªü d·∫°ng 2 vectors)

Sau ƒë√≥ ch√∫ng ta s·∫Ω d√πng m√¥ h√¨nh [RankNet] ƒë·ªÉ d·ª± ƒëo√°n th·ª≠ x√°c su·∫•t t√†i li·ªáu ( di ) li√™n quan v·ªõi truy v·∫•n ( q ) nhi·ªÅu h∆°n ( dj )
"""

if __name__ == "__main__":
    try:
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"S·ª≠ d·ª•ng device: {device}")

        # Load documents
        documents_path = "/content/cadao_tucngu_50_1.json"
        model_path = "/content/ranknet_model.pth"

        with open(documents_path, 'r', encoding='utf-8') as f:
            documents = json.load(f)

        # Kh·ªüi t·∫°o models
        sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')
        preparator = DataPreparator(sentence_model)

        # Load RankNet model
        sample_query_emb = sentence_model.encode(["test"])[0]
        input_size = len(sample_query_emb) * 2

        if os.path.exists(model_path):
            checkpoint = torch.load(model_path, map_location=device)
            fc1_weight_shape = checkpoint['fc1.weight'].shape
            fc2_weight_shape = checkpoint['fc2.weight'].shape
            hidden_size1 = fc1_weight_shape[0]
            hidden_size2 = fc2_weight_shape[0]

            model = RankNet(input_size=input_size, hidden_size1=hidden_size1,
                          hidden_size2=hidden_size2, dropout=0.5)
            model.load_state_dict(checkpoint)
            model.to(device)
            print(f"‚úÖ ƒê√£ load model t·ª´ {model_path}")
        else:
            print(f"‚ùå Kh√¥ng t√¨m th·∫•y model file: {model_path}")
            exit()

        # TEST C·ª§ TH·ªÇ
        query = "B√°n anh em xa"
        doc_id1 = 75
        doc_id2 = 107

        test_specific_document_pair(
            model=model,
            sentence_model=sentence_model,
            preparator=preparator,
            documents=documents,
            query=query,
            doc_id1=doc_id1,
            doc_id2=doc_id2,
            device=device
        )

    except Exception as e:
        print(f"Error: {e}")
        import traceback
        traceback.print_exc()