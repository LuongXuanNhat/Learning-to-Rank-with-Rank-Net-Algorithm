{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6h7KPFJqNMB",
        "outputId": "6857e162-b2a2-4ef6-aaf7-2645cf431b51"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-94b50cf1-dccd-03a1-ee83-9e458e7f4cfe)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import các thư viện cần sử dụng"
      ],
      "metadata": {
        "id": "yLS4WrSarXBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi\n",
        "!pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-GwQPsx2mEg",
        "outputId": "19cec9fb-8da5-48ae-f518-c1ee79605d93"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pyvi\n",
            "  Downloading pyvi-0.1.1-py2.py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.6.1)\n",
            "Collecting sklearn-crfsuite (from pyvi)\n",
            "  Downloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Collecting python-crfsuite>=0.9.7 (from sklearn-crfsuite->pyvi)\n",
            "  Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n",
            "Downloading pyvi-0.1.1-py2.py3-none-any.whl (8.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sklearn_crfsuite-0.5.0-py2.py3-none-any.whl (10 kB)\n",
            "Downloading python_crfsuite-0.9.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m77.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: python-crfsuite, sklearn-crfsuite, pyvi\n",
            "Successfully installed python-crfsuite-0.9.11 pyvi-0.1.1 sklearn-crfsuite-0.5.0\n",
            "Requirement already satisfied: sentence_transformers in /usr/local/lib/python3.11/dist-packages (5.0.0)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.55.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (2.6.0+cu124)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (1.16.1)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (0.34.3)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from sentence_transformers) (4.14.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2025.3.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (6.0.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (2.32.3)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.20.0->sentence_transformers) (1.1.7)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (0.6.2)\n",
            "Collecting nvidia-nccl-cu12==2.21.5 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11.0->sentence_transformers)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11.0->sentence_transformers) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.11.0->sentence_transformers) (1.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.21.4)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers<5.0.0,>=4.41.0->sentence_transformers) (0.6.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->sentence_transformers) (3.6.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11.0->sentence_transformers) (3.0.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.20.0->sentence_transformers) (2025.8.3)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m121.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m93.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.21.5-py3-none-manylinux2014_x86_64.whl (188.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m188.7/188.7 MB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m110.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-nccl-cu12\n",
            "    Found existing installation: nvidia-nccl-cu12 2.23.4\n",
            "    Uninstalling nvidia-nccl-cu12-2.23.4:\n",
            "      Successfully uninstalled nvidia-nccl-cu12-2.23.4\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nccl-cu12-2.21.5 nvidia-nvjitlink-cu12-12.4.127\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pyvi import ViTokenizer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "baNRYvdKrrKf"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Định nghĩa class RankNet - kết thừa từ thư viện: torch.nn.Module"
      ],
      "metadata": {
        "id": "i4FErFmyv87U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128, dropout=0.2):\n",
        "        \"\"\"\n",
        "        RankNet với 2 tầng ẩn\n",
        "\n",
        "        Args:\n",
        "            input_size (int): Số features đầu vào\n",
        "            hidden_size1 (int): Số neurons tầng ẩn thứ nhất\n",
        "            hidden_size2 (int): Số neurons tầng ẩn thứ hai\n",
        "            dropout (float): Tỷ lệ dropout để tránh overfitting\n",
        "        \"\"\"\n",
        "        super(RankNet, self).__init__()\n",
        "\n",
        "        # Định nghĩa các tầng\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer cho score\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Layer normalization thay vì batch normalization để tránh lỗi khi batch_size=1\n",
        "        self.ln1 = nn.LayerNorm(hidden_size1)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor có shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output scores có shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # Đảm bảo input là tensor và có đúng kiểu dữ liệu\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        # Tầng ẩn thứ nhất\n",
        "        x = self.fc1(x)\n",
        "        x = self.ln1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # Tầng ẩn thứ hai\n",
        "        x = self.fc2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict_rank(self, x1, x2):\n",
        "        \"\"\"\n",
        "        So sánh ranking giữa hai samples\n",
        "\n",
        "        Args:\n",
        "            x1, x2 (torch.Tensor): Hai samples cần so sánh\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Xác suất x1 được rank cao hơn x2\n",
        "        \"\"\"\n",
        "        score1 = self.forward(x1)\n",
        "        score2 = self.forward(x2)\n",
        "\n",
        "        # Sử dụng sigmoid để chuyển về xác suất\n",
        "        prob = torch.sigmoid(score1 - score2)\n",
        "        return prob\n"
      ],
      "metadata": {
        "id": "OPdkzRkzwIcW"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hàm mất mát"
      ],
      "metadata": {
        "id": "Zkp0aCK-w58i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ranknet_loss(s_i, s_j, P_ij):\n",
        "    diff = s_i - s_j\n",
        "    P_hat = torch.sigmoid(diff)  # Xác suất dự đoán P̂ᵢⱼ\n",
        "    # Thêm epsilon để tránh log(0)\n",
        "    epsilon = 1e-10\n",
        "    loss = -P_ij * torch.log(P_hat + epsilon) - (1 - P_ij) * torch.log(1 - P_hat + epsilon)\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "id": "D3YdLQ86w77_"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Một số lớp, hàm phụ trợ"
      ],
      "metadata": {
        "id": "c4zlhnATyWha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreparator:\n",
        "    \"\"\"\n",
        "    Class chuyên dụng cho việc chuẩn bị dữ liệu pairwise training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sentence_model: SentenceTransformer, stopwords_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Khởi tạo DataPreparator\n",
        "\n",
        "        Args:\n",
        "            sentence_model: Mô hình sentence transformer\n",
        "            stopwords_path: Đường dẫn file stopwords (optional)\n",
        "        \"\"\"\n",
        "        self.sentence_model = sentence_model\n",
        "        self.stopwords = set()\n",
        "\n",
        "        # Load stopwords nếu có\n",
        "        if stopwords_path and os.path.exists(stopwords_path):\n",
        "            with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "                self.stopwords = set(line.strip() for line in f if line.strip())\n",
        "            print(f\"Đã load {len(self.stopwords)} stopwords từ {stopwords_path}\")\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Tiền xử lý văn bản với PyVi\n",
        "\n",
        "        Args:\n",
        "            text: Văn bản cần xử lý\n",
        "\n",
        "        Returns:\n",
        "            str: Văn bản đã được xử lý\n",
        "        \"\"\"\n",
        "        # 1. Chuẩn hóa cơ bản\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)  # loại bỏ dấu câu\n",
        "        text = re.sub(r'\\d+', ' ', text)      # loại bỏ số\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # bỏ khoảng trắng thừa\n",
        "\n",
        "        # 2. Tokenize với PyVi\n",
        "        tokenized = ViTokenizer.tokenize(text)\n",
        "\n",
        "        # 3. Tách thành danh sách từ và lọc stopwords\n",
        "        tokens = tokenized.split()\n",
        "        if self.stopwords:\n",
        "            tokens = [token for token in tokens if token not in self.stopwords]\n",
        "\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "    def calculate_relevance_scores(self, query: str, documents: List[Dict],\n",
        "                                 relevance_method: str = 'cosine_ranking') -> List[Tuple[int, float, int]]:\n",
        "        \"\"\"\n",
        "        Tính điểm relevance cho tất cả documents với một query\n",
        "\n",
        "        Args:\n",
        "            query: Câu truy vấn\n",
        "            documents: Danh sách documents\n",
        "            relevance_method: Phương pháp tính relevance\n",
        "\n",
        "        Returns:\n",
        "            List[Tuple[int, float, int]]: (doc_index, similarity_score, relevance_label)\n",
        "        \"\"\"\n",
        "        # Preprocess query\n",
        "        processed_query = self.preprocess_text(query)\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = self.sentence_model.encode([processed_query])\n",
        "\n",
        "        # Preprocess và encode tất cả documents\n",
        "        doc_texts = []\n",
        "        for doc in documents:\n",
        "            processed_doc = self.preprocess_text(doc['value'])\n",
        "            doc_texts.append(processed_doc)\n",
        "\n",
        "        doc_embeddings = self.sentence_model.encode(doc_texts)\n",
        "\n",
        "        # Tính cosine similarity\n",
        "        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "        # Gán relevance labels\n",
        "        doc_relevances = []\n",
        "\n",
        "        if relevance_method == 'cosine_ranking':\n",
        "            # Sắp xếp theo similarity và gán labels theo ranking\n",
        "            doc_similarities = [(i, sim) for i, sim in enumerate(similarities)]\n",
        "            doc_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            for rank, (doc_idx, similarity) in enumerate(doc_similarities):\n",
        "                if rank < 3:  # Top 1-3: Highly relevant\n",
        "                    relevance = 3\n",
        "                elif rank < 7:  # Top 4-7: Relevant\n",
        "                    relevance = 2\n",
        "                elif rank < 12:  # Top 8-12: Somewhat relevant\n",
        "                    relevance = 1\n",
        "                else:  # Còn lại: Not relevant\n",
        "                    relevance = 0\n",
        "\n",
        "                doc_relevances.append((doc_idx, float(similarity), relevance))\n",
        "\n",
        "        elif relevance_method == 'threshold_based':\n",
        "            # Gán labels dựa trên threshold của similarity\n",
        "            for doc_idx, similarity in enumerate(similarities):\n",
        "                if similarity >= 0.8:\n",
        "                    relevance = 3\n",
        "                elif similarity >= 0.6:\n",
        "                    relevance = 2\n",
        "                elif similarity >= 0.4:\n",
        "                    relevance = 1\n",
        "                else:\n",
        "                    relevance = 0\n",
        "\n",
        "                doc_relevances.append((doc_idx, float(similarity), relevance))\n",
        "\n",
        "        return doc_relevances\n",
        "\n",
        "    def generate_pairwise_data(self, queries: List[str], documents: List[Dict],\n",
        "                             relevance_method: str = 'cosine_ranking',\n",
        "                             max_pairs_per_query: Optional[int] = None,\n",
        "                             balance_labels: bool = True) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Tạo dữ liệu pairwise training\n",
        "\n",
        "        Args:\n",
        "            queries: Danh sách queries\n",
        "            documents: Danh sách documents\n",
        "            relevance_method: Phương pháp tính relevance\n",
        "            max_pairs_per_query: Giới hạn số cặp per query (optional)\n",
        "            balance_labels: Có cân bằng labels không\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Danh sách pairwise samples\n",
        "        \"\"\"\n",
        "        print(f\"Bắt đầu tạo dữ liệu pairwise với {len(queries)} queries và {len(documents)} documents\")\n",
        "\n",
        "        all_pairwise_data = []\n",
        "        label_counts = {0.0: 0, 0.5: 0, 1.0: 0}\n",
        "\n",
        "        for query_idx, query in enumerate(tqdm(queries, desc=\"Tạo dữ liệu pairwise\")):\n",
        "            # Tính relevance scores\n",
        "            doc_relevances = self.calculate_relevance_scores(query, documents, relevance_method)\n",
        "\n",
        "            # Tạo tất cả các cặp possible\n",
        "            query_pairs = []\n",
        "\n",
        "            for i in range(len(doc_relevances)):\n",
        "                for j in range(i + 1, len(doc_relevances)):\n",
        "                    doc1_idx, doc1_sim, doc1_rel = doc_relevances[i]\n",
        "                    doc2_idx, doc2_sim, doc2_rel = doc_relevances[j]\n",
        "\n",
        "                    # Xác định label cho cặp\n",
        "                    if doc1_rel > doc2_rel:\n",
        "                        label = 1.0  # Doc1 tốt hơn Doc2\n",
        "                    elif doc1_rel < doc2_rel:\n",
        "                        label = 0.0  # Doc1 kém hơn Doc2\n",
        "                    else:\n",
        "                        label = 0.5  # Bằng nhau\n",
        "\n",
        "                    # Tạo sample\n",
        "                    pairwise_sample = {\n",
        "                        \"query_id\": query_idx,\n",
        "                        \"query\": query,\n",
        "                        \"query_processed\": self.preprocess_text(query),\n",
        "                        \"doc1_id\": documents[doc1_idx]['id'],\n",
        "                        \"doc1_text\": documents[doc1_idx]['value'],\n",
        "                        \"doc1_processed\": self.preprocess_text(documents[doc1_idx]['value']),\n",
        "                        \"doc1_relevance\": doc1_rel,\n",
        "                        \"doc1_similarity\": doc1_sim,\n",
        "                        \"doc2_id\": documents[doc2_idx]['id'],\n",
        "                        \"doc2_text\": documents[doc2_idx]['value'],\n",
        "                        \"doc2_processed\": self.preprocess_text(documents[doc2_idx]['value']),\n",
        "                        \"doc2_relevance\": doc2_rel,\n",
        "                        \"doc2_similarity\": doc2_sim,\n",
        "                        \"label\": label,\n",
        "                        \"relevance_method\": relevance_method\n",
        "                    }\n",
        "\n",
        "                    query_pairs.append(pairwise_sample)\n",
        "                    label_counts[label] += 1\n",
        "\n",
        "            # Giới hạn số cặp per query nếu cần\n",
        "            if max_pairs_per_query and len(query_pairs) > max_pairs_per_query:\n",
        "                # Sampling có cân bằng labels\n",
        "                if balance_labels:\n",
        "                    query_pairs = self._balanced_sampling(query_pairs, max_pairs_per_query)\n",
        "                else:\n",
        "                    query_pairs = query_pairs[:max_pairs_per_query]\n",
        "\n",
        "            all_pairwise_data.extend(query_pairs)\n",
        "\n",
        "        # In thống kê\n",
        "        total_pairs = len(all_pairwise_data)\n",
        "        print(f\"\\nThống kê dữ liệu pairwise:\")\n",
        "        print(f\"Tổng số cặp: {total_pairs}\")\n",
        "        print(f\"Label 0.0 (doc1 < doc2): {label_counts[0.0]} ({label_counts[0.0]/total_pairs*100:.1f}%)\")\n",
        "        print(f\"Label 0.5 (doc1 = doc2): {label_counts[0.5]} ({label_counts[0.5]/total_pairs*100:.1f}%)\")\n",
        "        print(f\"Label 1.0 (doc1 > doc2): {label_counts[1.0]} ({label_counts[1.0]/total_pairs*100:.1f}%)\")\n",
        "\n",
        "        return all_pairwise_data\n",
        "\n",
        "    def _balanced_sampling(self, pairs: List[Dict], max_pairs: int) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        Lấy mẫu cân bằng theo labels\n",
        "\n",
        "        Args:\n",
        "            pairs: Danh sách tất cả các cặp\n",
        "            max_pairs: Số cặp tối đa\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Danh sách cặp đã được sampling\n",
        "        \"\"\"\n",
        "        # Phân loại theo labels\n",
        "        label_groups = {0.0: [], 0.5: [], 1.0: []}\n",
        "        for pair in pairs:\n",
        "            label_groups[pair['label']].append(pair)\n",
        "\n",
        "        # Tính số mẫu cho mỗi label (cân bằng)\n",
        "        samples_per_label = max_pairs // 3\n",
        "        remaining = max_pairs % 3\n",
        "\n",
        "        sampled_pairs = []\n",
        "\n",
        "        # Lấy mẫu từng label\n",
        "        for i, (label, group) in enumerate(label_groups.items()):\n",
        "            n_samples = samples_per_label + (1 if i < remaining else 0)\n",
        "            n_samples = min(n_samples, len(group))  # Không vượt quá số có sẵn\n",
        "\n",
        "            # Random sampling\n",
        "            import random\n",
        "            sampled = random.sample(group, n_samples)\n",
        "            sampled_pairs.extend(sampled)\n",
        "\n",
        "        return sampled_pairs\n",
        "\n",
        "    def save_pairwise_data(self, pairwise_data: List[Dict], file_path: str,\n",
        "                          metadata: Optional[Dict] = None):\n",
        "        \"\"\"\n",
        "        Lưu dữ liệu pairwise vào file\n",
        "\n",
        "        Args:\n",
        "            pairwise_data: Dữ liệu pairwise\n",
        "            file_path: Đường dẫn file\n",
        "            metadata: Metadata bổ sung (optional)\n",
        "        \"\"\"\n",
        "        # Tạo thư mục nếu chưa có\n",
        "        os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else '.', exist_ok=True)\n",
        "\n",
        "        # Chuẩn bị data để lưu\n",
        "        data_to_save = {\n",
        "            \"metadata\": {\n",
        "                \"total_pairs\": len(pairwise_data),\n",
        "                \"created_by\": \"DataPreparator\",\n",
        "                \"format_version\": \"1.0\",\n",
        "                **(metadata or {})\n",
        "            },\n",
        "            \"pairwise_data\": pairwise_data\n",
        "        }\n",
        "\n",
        "        # Lưu file\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data_to_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Đã lưu {len(pairwise_data)} cặp dữ liệu vào {file_path}\")\n",
        "\n",
        "    def load_pairwise_data(self, file_path: str) -> Tuple[List[Dict], Dict]:\n",
        "        \"\"\"\n",
        "        Load dữ liệu pairwise từ file\n",
        "\n",
        "        Args:\n",
        "            file_path: Đường dẫn file\n",
        "\n",
        "        Returns:\n",
        "            Tuple[List[Dict], Dict]: (pairwise_data, metadata)\n",
        "        \"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File không tồn tại: {file_path}\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        pairwise_data = data.get('pairwise_data', [])\n",
        "        metadata = data.get('metadata', {})\n",
        "\n",
        "        print(f\"Đã load {len(pairwise_data)} cặp dữ liệu từ {file_path}\")\n",
        "\n",
        "        return pairwise_data, metadata\n",
        "def prepare_training_data(documents_path: str, queries: List[str],\n",
        "                         output_file: str = \"document_pairs.json\",\n",
        "                         sentence_model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "                         stopwords_path: Optional[str] = None,\n",
        "                         relevance_method: str = 'cosine_ranking',\n",
        "                         max_pairs_per_query: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    Hàm wrapper để chuẩn bị dữ liệu training một cách dễ dàng\n",
        "\n",
        "    Args:\n",
        "        documents_path: Đường dẫn file documents JSON\n",
        "        queries: Danh sách queries\n",
        "        output_file: Tên file output\n",
        "        sentence_model_name: Tên sentence transformer model\n",
        "        stopwords_path: Đường dẫn stopwords file\n",
        "        relevance_method: Phương pháp tính relevance\n",
        "        max_pairs_per_query: Giới hạn cặp per query\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: Dữ liệu pairwise đã tạo\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"CHUẨN BỊ DỮ LIỆU PAIRWISE CHO RANKNET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Load documents\n",
        "    print(f\"1. Đang load documents từ {documents_path}...\")\n",
        "    with open(documents_path, 'r', encoding='utf-8') as f:\n",
        "        documents = json.load(f)\n",
        "    print(f\"   ✓ Đã load {len(documents)} documents\")\n",
        "\n",
        "    # 2. Khởi tạo sentence transformer\n",
        "    print(f\"2. Đang khởi tạo sentence transformer: {sentence_model_name}...\")\n",
        "    sentence_model = SentenceTransformer(sentence_model_name)\n",
        "    print(f\"   ✓ Model đã sẵn sàng\")\n",
        "\n",
        "    # 3. Khởi tạo DataPreparator\n",
        "    print(\"3. Đang khởi tạo DataPreparator...\")\n",
        "    preparator = DataPreparator(sentence_model, stopwords_path)\n",
        "    print(f\"   ✓ DataPreparator đã sẵn sàng\")\n",
        "\n",
        "    # 4. Tạo pairwise data\n",
        "    print(\"4. Đang tạo dữ liệu pairwise...\")\n",
        "    pairwise_data = preparator.generate_pairwise_data(\n",
        "        queries=queries,\n",
        "        documents=documents,\n",
        "        relevance_method=relevance_method,\n",
        "        max_pairs_per_query=max_pairs_per_query,\n",
        "        balance_labels=True\n",
        "    )\n",
        "    print(f\"   ✓ Đã tạo {len(pairwise_data)} cặp dữ liệu\")\n",
        "\n",
        "    # 5. Lưu file\n",
        "    print(f\"5. Đang lưu vào {output_file}...\")\n",
        "    metadata = {\n",
        "        \"documents_path\": documents_path,\n",
        "        \"num_queries\": len(queries),\n",
        "        \"queries\": queries,\n",
        "        \"relevance_method\": relevance_method,\n",
        "        \"sentence_model\": sentence_model_name,\n",
        "        \"max_pairs_per_query\": max_pairs_per_query\n",
        "    }\n",
        "\n",
        "    preparator.save_pairwise_data(pairwise_data, output_file, metadata)\n",
        "    print(f\"   ✓ Dữ liệu đã được lưu thành công\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"HOÀN THÀNH CHUẨN BỊ DỮ LIỆU!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return pairwise_data"
      ],
      "metadata": {
        "id": "8Q-8K8nHKJvX"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chuẩn bị dữ liệu, lấy ra so sánh\n",
        "class RankNetDataset(Dataset):\n",
        "    \"\"\"Dataset cho RankNet training\"\"\"\n",
        "    def __init__(self, pairwise_data, sentence_model):\n",
        "        self.data = pairwise_data\n",
        "        self.sentence_model = sentence_model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    #  Trả về một dictionary chứa feature1, feature2, và label\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Encode query và documents\n",
        "        query_emb = self.sentence_model.encode([sample['query']])[0]\n",
        "        doc1_emb = self.sentence_model.encode([sample['doc1_text']])[0]\n",
        "        doc2_emb = self.sentence_model.encode([sample['doc2_text']])[0]\n",
        "\n",
        "        # Tạo features bằng cách concat query+doc\n",
        "        feature1 = np.concatenate([query_emb, doc1_emb])\n",
        "        feature2 = np.concatenate([query_emb, doc2_emb])\n",
        "\n",
        "        return {\n",
        "            'feature1': torch.tensor(feature1, dtype=torch.float32),\n",
        "            'feature2': torch.tensor(feature2, dtype=torch.float32),\n",
        "            'label': torch.tensor(sample['label'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_ranking(model, documents, queries, sentence_model, preparator, device, top_k=10):\n",
        "    \"\"\"\n",
        "    Đánh giá mô hình bằng cách rank lại documents cho mỗi query\n",
        "    và tính pairwise accuracy\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for query in queries:\n",
        "            # Tiền xử lý query\n",
        "            processed_query_tokens = preparator.preprocess_text(query)\n",
        "            processed_query = \" \".join(processed_query_tokens)\n",
        "\n",
        "            # Encode query\n",
        "            query_emb = sentence_model.encode([processed_query])[0]\n",
        "\n",
        "            # Score tất cả documents\n",
        "            doc_scores = []\n",
        "            for doc in documents:\n",
        "                processed_doc_tokens = preparator.preprocess_text(doc['value'])\n",
        "                processed_doc = \" \".join(processed_doc_tokens)\n",
        "\n",
        "                doc_emb = sentence_model.encode([processed_doc])[0]\n",
        "\n",
        "                feature = torch.tensor(\n",
        "                    np.concatenate([query_emb, doc_emb]),\n",
        "                    dtype=torch.float32\n",
        "                ).unsqueeze(0).to(device)\n",
        "\n",
        "                score = model(feature).item()\n",
        "                doc_scores.append((doc['id'], score, doc))\n",
        "\n",
        "            # Sắp xếp giảm dần theo score\n",
        "            doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Lấy top k\n",
        "            top_docs = doc_scores[:top_k]\n",
        "\n",
        "            results[query] = {\n",
        "                'top_documents': [(doc_id, doc['value'], score) for doc_id, score, doc in top_docs],\n",
        "                'all_scores': doc_scores\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QpI7PnRkybGn"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hàm huấn luyện\n",
        "def train_ranknet(model, dataloader, optimizer, device, num_epochs=20,\n",
        "                          loss_type='cross_entropy', patience=5, min_delta=1e-4):\n",
        "    \"\"\"\n",
        "    Huấn luyện mô hình RankNet với các cải tiến\n",
        "\n",
        "    Args:\n",
        "        model: RankNet model\n",
        "        dataloader: DataLoader\n",
        "        optimizer: optimizer\n",
        "        device: cuda/cpu\n",
        "        num_epochs: số epochs\n",
        "        loss_type: loại loss function\n",
        "        patience: số epochs chờ để early stopping\n",
        "        min_delta: threshold nhỏ nhất để coi là improvement\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            feature1 = batch['feature1'].to(device)\n",
        "            feature2 = batch['feature2'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            score1 = model(feature1).squeeze()\n",
        "            score2 = model(feature2).squeeze()\n",
        "\n",
        "            # Tính loss với hàm cải tiến\n",
        "            loss = ranknet_loss(score1, score2, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping để tránh exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f\"{loss.item():.4f}\",\n",
        "                'LR': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        loss_history.append(avg_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_loss < best_loss - min_delta:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_ranknet_model.pth')\n",
        "            print(f\"New best model saved with loss: {best_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            print(f\"Loading best model with loss: {best_loss:.4f}\")\n",
        "            model.load_state_dict(torch.load('best_ranknet_model.pth'))\n",
        "            break\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KcoSWYiDK4IR"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hàm đánh giá kết quả\n",
        "def calculate_pairwise_accuracy(model, test_data, sentence_model, device):\n",
        "    \"\"\"Tính Pairwise Accuracy\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in tqdm(test_data, desc=\"Tính Pairwise Accuracy\"):\n",
        "            # Encode features\n",
        "            query_emb = sentence_model.encode([sample['query']])[0]\n",
        "            doc1_emb = sentence_model.encode([sample['doc1_text']])[0]\n",
        "            doc2_emb = sentence_model.encode([sample['doc2_text']])[0]\n",
        "\n",
        "            feature1 = torch.tensor(np.concatenate([query_emb, doc1_emb]),\n",
        "                                  dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            feature2 = torch.tensor(np.concatenate([query_emb, doc2_emb]),\n",
        "                                  dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            # Tính scores\n",
        "            score1 = model(feature1).item()\n",
        "            score2 = model(feature2).item()\n",
        "\n",
        "            # Dự đoán\n",
        "            if sample['label'] == 1.0:  # doc1 should be ranked higher\n",
        "                if score1 > score2:\n",
        "                    correct_predictions += 1\n",
        "            elif sample['label'] == 0.0:  # doc2 should be ranked higher\n",
        "                if score2 > score1:\n",
        "                    correct_predictions += 1\n",
        "            else:  # Equal relevance (label = 0.5)\n",
        "                # Coi như đúng nếu difference nhỏ\n",
        "                if abs(score1 - score2) < 0.1:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "            total_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy"
      ],
      "metadata": {
        "id": "CFhp1xa9MT7Y"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Chạy chương trình** Tính toán chính"
      ],
      "metadata": {
        "id": "nX9zBcDTydNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Thiết lập device\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Sử dụng device: {device}\")\n",
        "\n",
        "        # Bước 1: Tải tài liệu và khởi tạo sentence-transformer\n",
        "        documents_path = \"/content/cadao_tucngu_50_1.json\"\n",
        "        pairwise_data_file = \"/content/document_pairs.json\"\n",
        "        # documents_path = \"FetchData/train/anh_em_mot_nha/data.json\"\n",
        "\n",
        "        # Chạy document để lấy đánh giá với kết quả\n",
        "        with open(documents_path, 'r', encoding='utf-8') as f:\n",
        "            documents = json.load(f)\n",
        "        stopwords_path = \"/content/vietnamese-stopwords.txt\"\n",
        "        # Tạo cặp dữ liệu huấn luyện - Cải thiện với multiple queries\n",
        "        queries = [\n",
        "            \"Tình cảm anh em\",\n",
        "            \"tấm gương hiếu thảo\",\n",
        "            \"tình yêu thương ba mẹ\",\n",
        "            \"lòng biết ơn cha mẹ\"\n",
        "        ]\n",
        "\n",
        "        if os.path.exists(pairwise_data_file):\n",
        "            print(f\"1. Load dữ liệu pairwise từ {pairwise_data_file}...\")\n",
        "\n",
        "            # Khởi tạo sentence model trước\n",
        "            sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "            preparator = DataPreparator(sentence_model, stopwords_path)\n",
        "\n",
        "            # Load dữ liệu\n",
        "            pairwise_data, metadata = preparator.load_pairwise_data(pairwise_data_file)\n",
        "            print(f\"   ✓ Đã load {len(pairwise_data)} cặp từ file có sẵn\")\n",
        "            print(f\"   ✓ Metadata: {metadata}\")\n",
        "\n",
        "        else:\n",
        "            print(\"1. Tạo dữ liệu pairwise mới...\")\n",
        "\n",
        "            # Tạo dữ liệu pairwise\n",
        "            pairwise_data = prepare_training_data(\n",
        "                documents_path=documents_path,\n",
        "                queries=queries,\n",
        "                output_file=pairwise_data_file,\n",
        "                stopwords_path=stopwords_path,\n",
        "                relevance_method=\"cosine_ranking\",\n",
        "                max_pairs_per_query=1000\n",
        "            )\n",
        "\n",
        "            # Khởi tạo sentence model\n",
        "            sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') #384 neurons\n",
        "        # Tạo dataset và dataloader\n",
        "        print(\"Đang chuẩn bị dataset...\")\n",
        "        dataset = RankNetDataset(pairwise_data, sentence_model)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "\n",
        "        # Bước 2: Khởi tạo và huấn luyện mô hình RankNet\n",
        "        print(\"Đang khởi tạo mô hình RankNet...\")\n",
        "\n",
        "        # Tính kích thước input (query embedding + document embedding)\n",
        "        sample_query_emb = sentence_model.encode([\"test\"])[0]\n",
        "        input_size = len(sample_query_emb) * 2  # query + document embeddings\n",
        "        print(f\"Input size: {input_size}\")\n",
        "\n",
        "        model = RankNet(input_size=input_size, hidden_size1=128, hidden_size2=64, dropout=0.5)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "        print(\"Bắt đầu huấn luyện mô hình...\")\n",
        "        model = train_ranknet(model, dataloader, optimizer, device, num_epochs=20)\n",
        "\n",
        "        # Lưu mô hình\n",
        "        torch.save(model.state_dict(), 'ranknet_model.pth')\n",
        "        print(\"Đã lưu mô hình vào ranknet_model.pth\")\n",
        "\n",
        "                # Bước 3: Đánh giá độ chính xác của mô hình\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ĐÁNH GIÁ MÔ HÌNH\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Test queries\n",
        "        test_queries = [\n",
        "            \"tình cảm anh em\",\n",
        "            \"tình yêu thương ba mẹ\",\n",
        "        ]\n",
        "\n",
        "        print(\"Đang đánh giá ranking...\")\n",
        "        ranking_results = evaluate_ranking(model, documents, test_queries, sentence_model, preparator, device, top_k=10)\n",
        "\n",
        "\n",
        "        # Hiển thị kết quả top 10 cho mỗi query\n",
        "        for query, result in ranking_results.items():\n",
        "            print(f\"\\nQuery: '{query}'\")\n",
        "            print(\"Top 10 documents:\")\n",
        "            for i, (doc_id, doc_text, score) in enumerate(result['top_documents'], 1):\n",
        "                print(f\"{i:2d}. [ID: {doc_id}] Score: {score:.4f}\")\n",
        "                print(f\"    Text: {doc_text[:100]}...\")\n",
        "                print()\n",
        "\n",
        "        # Tính Pairwise Accuracy trên tập test\n",
        "        print(\"Đang tính Pairwise Accuracy...\")\n",
        "\n",
        "        # Tạo test data từ test queries\n",
        "        test_pairwise_data = prepare_training_data(\n",
        "                documents_path=documents_path,\n",
        "                queries=queries,\n",
        "                output_file=pairwise_data_file,\n",
        "                stopwords_path=stopwords_path,\n",
        "                relevance_method=\"cosine_ranking\",\n",
        "                max_pairs_per_query=1000\n",
        "            )  # Giới hạn để tính nhanh\n",
        "        accuracy = calculate_pairwise_accuracy(model, test_pairwise_data, sentence_model, device)\n",
        "\n",
        "        print(f\"\\nPairwise Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"HOÀN THÀNH!\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n"
      ],
      "metadata": {
        "id": "LpgjMrNrMpLx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "0c1f306e-0464-496d-876c-d332ed6d09ac"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sử dụng device: cuda\n",
            "1. Load dữ liệu pairwise từ /content/document_pairs.json...\n",
            "Đã load 1942 stopwords từ /content/vietnamese-stopwords.txt\n",
            "Đã load 2664 cặp dữ liệu từ /content/document_pairs.json\n",
            "   ✓ Đã load 2664 cặp từ file có sẵn\n",
            "   ✓ Metadata: {'total_pairs': 2664, 'created_by': 'DataPreparator', 'format_version': '1.0', 'documents_path': '/content/cadao_tucngu_50_1.json', 'num_queries': 4, 'queries': ['Tình cảm anh em', 'tấm gương hiếu thảo', 'tình yêu thương ba mẹ', 'lòng biết ơn cha mẹ'], 'relevance_method': 'cosine_ranking', 'sentence_model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', 'max_pairs_per_query': 1000}\n",
            "Đang chuẩn bị dataset...\n",
            "Đang khởi tạo mô hình RankNet...\n",
            "Input size: 768\n",
            "Bắt đầu huấn luyện mô hình...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/20: 100%|██████████| 84/84 [01:23<00:00,  1.01it/s, Loss=0.6004, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/20, Average Loss: 0.6654\n",
            "New best model saved with loss: 0.6654\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/20: 100%|██████████| 84/84 [01:23<00:00,  1.01it/s, Loss=0.5259, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/20, Average Loss: 0.5929\n",
            "New best model saved with loss: 0.5929\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/20: 100%|██████████| 84/84 [01:27<00:00,  1.04s/it, Loss=0.3155, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/20, Average Loss: 0.5476\n",
            "New best model saved with loss: 0.5476\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/20: 100%|██████████| 84/84 [01:23<00:00,  1.01it/s, Loss=0.6842, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/20, Average Loss: 0.5195\n",
            "New best model saved with loss: 0.5195\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/20: 100%|██████████| 84/84 [01:22<00:00,  1.02it/s, Loss=0.4710, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/20, Average Loss: 0.4989\n",
            "New best model saved with loss: 0.4989\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/20: 100%|██████████| 84/84 [01:24<00:00,  1.01s/it, Loss=0.4226, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/20, Average Loss: 0.4743\n",
            "New best model saved with loss: 0.4743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/20: 100%|██████████| 84/84 [01:25<00:00,  1.02s/it, Loss=0.4492, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/20, Average Loss: 0.4653\n",
            "New best model saved with loss: 0.4653\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/20: 100%|██████████| 84/84 [01:22<00:00,  1.02it/s, Loss=0.6278, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/20, Average Loss: 0.4565\n",
            "New best model saved with loss: 0.4565\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/20:  31%|███       | 26/84 [00:26<01:00,  1.04s/it, Loss=0.3936, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3117073294.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bắt đầu huấn luyện mô hình...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 68\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_ranknet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;31m# Lưu mô hình\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-504784067.py\u001b[0m in \u001b[0;36mtrain_ranknet\u001b[0;34m(model, dataloader, optimizer, device, num_epochs, loss_type, patience, min_delta)\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mprogress_bar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mf\"Epoch {epoch+1}/{num_epochs}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogress_bar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m             \u001b[0mfeature1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature1'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m             \u001b[0mfeature2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'feature2'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/tqdm/std.py\u001b[0m in \u001b[0;36m__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1180\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1181\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32min\u001b[0m \u001b[0miterable\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1182\u001b[0m                 \u001b[0;32myield\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m                 \u001b[0;31m# Update and possibly print the progressbar.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0;31m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    709\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             if (\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 764\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    765\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory_device\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getitems__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m                 \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-4037947825.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m# Encode query và documents\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mquery_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'query'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0mdoc1_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc1_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mdoc2_emb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentence_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'doc2_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mencode\u001b[0;34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[0m\n\u001b[1;32m   1050\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1051\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1052\u001b[0;31m                 \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1053\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"hpu\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1054\u001b[0m                     \u001b[0mout_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/SentenceTransformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, **kwargs)\u001b[0m\n\u001b[1;32m   1131\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule_kwarg_keys\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"forward_kwargs\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m                 }\n\u001b[0;32m-> 1133\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mmodule_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1134\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/sentence_transformers/models/Transformer.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, features, **kwargs)\u001b[0m\n\u001b[1;32m    435\u001b[0m         }\n\u001b[1;32m    436\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 437\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrans_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    438\u001b[0m         \u001b[0mtoken_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    439\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"token_embeddings\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtoken_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m   1004\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1006\u001b[0;31m         encoder_outputs = self.encoder(\n\u001b[0m\u001b[1;32m   1007\u001b[0m             \u001b[0membedding_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1008\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict, cache_position)\u001b[0m\n\u001b[1;32m    651\u001b[0m             \u001b[0mlayer_head_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhead_mask\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mhead_mask\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    652\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 653\u001b[0;31m             layer_outputs = layer_module(\n\u001b[0m\u001b[1;32m    654\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    655\u001b[0m                 \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mcache_position\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    561\u001b[0m     ) -> tuple[torch.Tensor]:\n\u001b[0;32m--> 562\u001b[0;31m         self_attention_outputs = self.attention(\n\u001b[0m\u001b[1;32m    563\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, past_key_value, output_attentions, cache_position)\u001b[0m\n\u001b[1;32m    500\u001b[0m             \u001b[0mcache_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_position\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    501\u001b[0m         )\n\u001b[0;32m--> 502\u001b[0;31m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    503\u001b[0m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# add attentions if we output them\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    504\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/bert/modeling_bert.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    442\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    443\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 444\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLayerNorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0minput_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    445\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    446\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1737\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1738\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1739\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1741\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1748\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1749\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1750\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1751\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1752\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/modules/normalization.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m         return F.layer_norm(\n\u001b[0m\u001b[1;32m    218\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlayer_norm\u001b[0;34m(input, normalized_shape, weight, bias, eps)\u001b[0m\n\u001b[1;32m   2908\u001b[0m             \u001b[0meps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2909\u001b[0m         )\n\u001b[0;32m-> 2910\u001b[0;31m     return torch.layer_norm(\n\u001b[0m\u001b[1;32m   2911\u001b[0m         \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnormalized_shape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackends\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcudnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2912\u001b[0m     )\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tiến hành kiểm thử mô hình [RankNet] đã được huấn luyện, chúng ta sẽ kiểm tra đơn giản bằng cách - đầu vào chúng ta sẽ cho 2 tài liệu ( di ) và ( dj ) (ở dạng 2 vectors)\n",
        "\n",
        "Sau đó chúng ta sẽ dùng mô hình [RankNet] để dự đoán thử xác suất tài liệu ( di ) liên quan với truy vấn ( q ) nhiều hơn ( dj )"
      ],
      "metadata": {
        "id": "dhEk7slbCqBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"Tiến hành kiểm thử mô hình [RankNet] đã được huấn luyện\"\"\"\n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Thiết lập device\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"Sử dụng device: {device}\")\n",
        "\n",
        "        # Bước 1: Tải tài liệu và khởi tạo sentence-transformer\n",
        "        documents_path = \"/content/cadao_tucngu_50_1.json\"\n",
        "        pairwise_data_file = \"/content/document_pairs.json\"\n",
        "        vector_pairs_file = \"/content/vector_pairs.json\"\n",
        "        model_path = \"/content/ranknet_model.pth\"  # Đường dẫn mô hình đã huấn luyện\n",
        "\n",
        "        # Load documents\n",
        "        with open(documents_path, 'r', encoding='utf-8') as f:\n",
        "            documents = json.load(f)\n",
        "        stopwords_path = \"/content/vietnamese-stopwords.txt\"\n",
        "\n",
        "        # Khởi tạo sentence model\n",
        "        sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        preparator = DataPreparator(sentence_model, stopwords_path)\n",
        "\n",
        "        # Load dữ liệu pairwise nếu có\n",
        "        if os.path.exists(pairwise_data_file):\n",
        "            print(f\"1. Load dữ liệu pairwise từ {pairwise_data_file}...\")\n",
        "            pairwise_data, metadata = preparator.load_pairwise_data(pairwise_data_file)\n",
        "            print(f\"   ✓ Đã load {len(pairwise_data)} cặp từ file có sẵn\")\n",
        "        else:\n",
        "            print(\"   ✗ Không tìm thấy file pairwise data\")\n",
        "            raise FileNotFoundError(f\"File {pairwise_data_file} không tồn tại\")\n",
        "\n",
        "        # Bước 2: Tạo và lưu vector pairs vào file\n",
        "        print(\"2. Tạo vector pairs và lưu vào file...\")\n",
        "\n",
        "        if len(pairwise_data) > 0:\n",
        "            sample_pair = pairwise_data[0]  # Lấy cặp đầu tiên\n",
        "\n",
        "            # Tạo embeddings cho query và documents\n",
        "            query_embedding = sentence_model.encode([sample_pair['query']])[0]\n",
        "            doc1_embedding = sentence_model.encode([sample_pair['doc1_text']])[0]  # Sửa key\n",
        "            doc2_embedding = sentence_model.encode([sample_pair['doc2_text']])[0]  # Sửa key\n",
        "\n",
        "            # Tạo combined vectors (query + document)\n",
        "            di = np.concatenate([query_embedding, doc1_embedding])\n",
        "            dj = np.concatenate([query_embedding, doc2_embedding])\n",
        "\n",
        "            # Tạo dictionary để lưu\n",
        "            vector_data = {\n",
        "                \"query\": sample_pair['query'],\n",
        "                \"doc1_text\": sample_pair['doc1_text'],\n",
        "                \"doc2_text\": sample_pair['doc2_text'],\n",
        "                \"doc1_relevance\": sample_pair['doc1_relevance'],\n",
        "                \"doc2_relevance\": sample_pair['doc2_relevance'],\n",
        "                \"label\": sample_pair['label'],\n",
        "                \"vector_di\": di.tolist(),\n",
        "                \"vector_dj\": dj.tolist(),\n",
        "                \"vector_size\": len(di),\n",
        "                \"query_embedding_size\": len(query_embedding),\n",
        "                \"doc_embedding_size\": len(doc1_embedding)\n",
        "            }\n",
        "\n",
        "            # Lưu vào file JSON\n",
        "            with open(vector_pairs_file, 'w', encoding='utf-8') as f:\n",
        "                json.dump(vector_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "            print(f\"   ✓ Đã lưu vector pairs vào {vector_pairs_file}\")\n",
        "            print(f\"   ✓ Query: '{sample_pair['query']}'\")\n",
        "            print(f\"   ✓ Document 1 (relevance: {sample_pair['doc1_relevance']}): '{sample_pair['doc1_text'][:100]}...'\")\n",
        "            print(f\"   ✓ Document 2 (relevance: {sample_pair['doc2_relevance']}): '{sample_pair['doc2_text'][:100]}...'\")\n",
        "            print(f\"   ✓ Label: {sample_pair['label']} (1.0: doc1 > doc2, 0.0: doc1 < doc2, 0.5: equal)\")\n",
        "            print(f\"   ✓ Vector size: {len(di)}\")\n",
        "\n",
        "        else:\n",
        "            print(\"   ✗ Không có dữ liệu pairwise để tạo vector\")\n",
        "            raise ValueError(\"Không có dữ liệu pairwise\")\n",
        "\n",
        "        # Bước 3: Đọc vector pairs từ file và kiểm tra\n",
        "        print(\"\\n3. Đọc vector pairs từ file và kiểm tra...\")\n",
        "\n",
        "        if os.path.exists(vector_pairs_file):\n",
        "            with open(vector_pairs_file, 'r', encoding='utf-8') as f:\n",
        "                loaded_data = json.load(f)\n",
        "\n",
        "            # Chuyển đổi lại thành numpy arrays\n",
        "            di_loaded = np.array(loaded_data['vector_di'])\n",
        "            dj_loaded = np.array(loaded_data['vector_dj'])\n",
        "\n",
        "            print(f\"   ✓ Đã load vectors từ file\")\n",
        "            print(f\"   ✓ Query: '{loaded_data['query']}'\")\n",
        "            print(f\"   ✓ Document 1 relevance: {loaded_data['doc1_relevance']}\")\n",
        "            print(f\"   ✓ Document 2 relevance: {loaded_data['doc2_relevance']}\")\n",
        "            print(f\"   ✓ Ground truth label: {loaded_data['label']}\")\n",
        "            print(f\"   ✓ Vector shape: {di_loaded.shape}\")\n",
        "\n",
        "            # Tính cosine similarity giữa hai vectors\n",
        "            dot_product = np.dot(di_loaded, dj_loaded)\n",
        "            norm_di = np.linalg.norm(di_loaded)\n",
        "            norm_dj = np.linalg.norm(dj_loaded)\n",
        "            cosine_sim = dot_product / (norm_di * norm_dj)\n",
        "            print(f\"   ✓ Cosine similarity giữa di và dj: {cosine_sim:.4f}\")\n",
        "\n",
        "        else:\n",
        "            print(f\"   ✗ Không tìm thấy file {vector_pairs_file}\")\n",
        "            raise FileNotFoundError(f\"File {vector_pairs_file} không tồn tại\")\n",
        "\n",
        "        # Bước 4: Khởi tạo và load mô hình RankNet đã huấn luyện\n",
        "        print(\"\\n4. Khởi tạo và load mô hình RankNet...\")\n",
        "\n",
        "        # Tính kích thước input\n",
        "        sample_query_emb = sentence_model.encode([\"test\"])[0]\n",
        "        input_size = len(sample_query_emb) * 2\n",
        "        print(f\"   ✓ Input size: {input_size}\")\n",
        "\n",
        "        # Load weights và tự động detect architecture\n",
        "        if os.path.exists(model_path):\n",
        "            # Load checkpoint để kiểm tra kích thước\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "\n",
        "            # Detect architecture từ checkpoint\n",
        "            fc1_weight_shape = checkpoint['fc1.weight'].shape\n",
        "            fc2_weight_shape = checkpoint['fc2.weight'].shape\n",
        "\n",
        "            hidden_size1 = fc1_weight_shape[0]  # output của fc1\n",
        "            hidden_size2 = fc2_weight_shape[0]  # output của fc2\n",
        "\n",
        "            print(f\"   ✓ Detected architecture: hidden_size1={hidden_size1}, hidden_size2={hidden_size2}\")\n",
        "\n",
        "            # Khởi tạo mô hình với đúng architecture\n",
        "            model = RankNet(input_size=input_size, hidden_size1=hidden_size1, hidden_size2=hidden_size2, dropout=0.5)\n",
        "            model.to(device)\n",
        "\n",
        "            # Load weights\n",
        "            model.load_state_dict(checkpoint)\n",
        "            model.eval()\n",
        "            print(f\"   ✓ Đã load mô hình từ {model_path}\")\n",
        "        else:\n",
        "            print(f\"   ⚠ Không tìm thấy mô hình tại {model_path}, sử dụng mô hình mặc định\")\n",
        "            # Khởi tạo mô hình mặc định\n",
        "            model = RankNet(input_size=input_size, hidden_size1=256, hidden_size2=128, dropout=0.2)\n",
        "            model.to(device)\n",
        "            print(f\"   ✓ Khởi tạo mô hình mặc định với hidden_size1=256, hidden_size2=128\")\n",
        "\n",
        "        # Bước 5: Test mô hình với vectors\n",
        "        print(\"\\n5. Test mô hình RankNet...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            # Chuyển vectors thành tensor\n",
        "            di_tensor = torch.tensor(di_loaded, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            dj_tensor = torch.tensor(dj_loaded, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            # Tính scores cho từng document\n",
        "            score_i = model(di_tensor).item()\n",
        "            score_j = model(dj_tensor).item()\n",
        "\n",
        "            # Tính xác suất P(di > dj)\n",
        "            diff = score_i - score_j\n",
        "            probability_i_better_j = torch.sigmoid(torch.tensor(diff)).item()\n",
        "\n",
        "            print(f\"   ✓ Score document i: {score_i:.4f}\")\n",
        "            print(f\"   ✓ Score document j: {score_j:.4f}\")\n",
        "            print(f\"   ✓ Score difference (i - j): {diff:.4f}\")\n",
        "            print(f\"   ✓ P(document i > document j): {probability_i_better_j:.4f} ({probability_i_better_j*100:.1f}%)\")\n",
        "\n",
        "            # So sánh với ground truth\n",
        "            ground_truth_label = loaded_data['label']\n",
        "\n",
        "            if ground_truth_label == 1.0:\n",
        "                expected = \"Document i should be ranked higher than document j\"\n",
        "                correct = probability_i_better_j > 0.5\n",
        "            elif ground_truth_label == 0.0:\n",
        "                expected = \"Document j should be ranked higher than document i\"\n",
        "                correct = probability_i_better_j < 0.5\n",
        "            else:  # label == 0.5\n",
        "                expected = \"Documents i and j should have similar ranks\"\n",
        "                correct = abs(probability_i_better_j - 0.5) < 0.1\n",
        "\n",
        "            print(f\"\\n   📊 ĐÁNH GIÁ KẾT QUẢ:\")\n",
        "            print(f\"   ✓ Ground truth: {expected}\")\n",
        "            print(f\"   ✓ Model prediction: Document i có {probability_i_better_j*100:.1f}% khả năng rank cao hơn document j\")\n",
        "            print(f\"   ✓ Prediction {'ĐÚNG' if correct else 'SAI'}: {'✓' if correct else '✗'}\")\n",
        "\n",
        "            # Thêm phân tích chi tiết\n",
        "            print(f\"\\n   📈 PHÂN TÍCH CHI TIẾT:\")\n",
        "            print(f\"   ✓ Document 1 relevance: {loaded_data['doc1_relevance']} -> Score: {score_i:.4f}\")\n",
        "            print(f\"   ✓ Document 2 relevance: {loaded_data['doc2_relevance']} -> Score: {score_j:.4f}\")\n",
        "\n",
        "            if loaded_data['doc1_relevance'] > loaded_data['doc2_relevance']:\n",
        "                relevance_says = \"Document 1 should rank higher\"\n",
        "            elif loaded_data['doc1_relevance'] < loaded_data['doc2_relevance']:\n",
        "                relevance_says = \"Document 2 should rank higher\"\n",
        "            else:\n",
        "                relevance_says = \"Documents should rank equally\"\n",
        "\n",
        "            model_says = \"Document 1 ranks higher\" if score_i > score_j else \"Document 2 ranks higher\" if score_j > score_i else \"Documents rank equally\"\n",
        "\n",
        "            print(f\"   ✓ Based on relevance: {relevance_says}\")\n",
        "            print(f\"   ✓ Based on model: {model_says}\")\n",
        "\n",
        "        # Bước 6: Test với nhiều cặp khác (optional)\n",
        "        print(\"\\n6. Test với nhiều cặp khác...\")\n",
        "\n",
        "        num_test_pairs = min(5, len(pairwise_data))\n",
        "        correct_predictions = 0\n",
        "\n",
        "        for i in range(num_test_pairs):\n",
        "            test_pair = pairwise_data[i]\n",
        "\n",
        "            # Tạo embeddings\n",
        "            query_emb = sentence_model.encode([test_pair['query']])[0]\n",
        "            doc1_emb = sentence_model.encode([test_pair['doc1_text']])[0]\n",
        "            doc2_emb = sentence_model.encode([test_pair['doc2_text']])[0]\n",
        "\n",
        "            # Tạo features\n",
        "            feature1 = torch.tensor(np.concatenate([query_emb, doc1_emb]),\n",
        "                                   dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            feature2 = torch.tensor(np.concatenate([query_emb, doc2_emb]),\n",
        "                                   dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                score1 = model(feature1).item()\n",
        "                score2 = model(feature2).item()\n",
        "                prob = torch.sigmoid(torch.tensor(score1 - score2)).item()\n",
        "\n",
        "                # Kiểm tra prediction\n",
        "                label = test_pair['label']\n",
        "                if label == 1.0 and prob > 0.5:\n",
        "                    correct_predictions += 1\n",
        "                elif label == 0.0 and prob < 0.5:\n",
        "                    correct_predictions += 1\n",
        "                elif label == 0.5 and abs(prob - 0.5) < 0.1:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "                print(f\"   Pair {i+1}: P(doc1 > doc2) = {prob:.3f}, Label = {label}, {'✓' if (label == 1.0 and prob > 0.5) or (label == 0.0 and prob < 0.5) or (label == 0.5 and abs(prob - 0.5) < 0.1) else '✗'}\")\n",
        "\n",
        "        accuracy = correct_predictions / num_test_pairs\n",
        "        print(f\"\\n   📊 Accuracy trên {num_test_pairs} cặp test: {accuracy:.2f} ({accuracy*100:.1f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*60)\n",
        "        print(\"HOÀN THÀNH KIỂM THỬ MÔ HÌNH RANKNET!\")\n",
        "        print(\"=\"*60)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WlKuK5xCuSu",
        "outputId": "a930060f-4e1f-417f-87e3-42838fcf779e"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sử dụng device: cuda\n",
            "Đã load 1942 stopwords từ /content/vietnamese-stopwords.txt\n",
            "1. Load dữ liệu pairwise từ /content/document_pairs.json...\n",
            "Đã load 2664 cặp dữ liệu từ /content/document_pairs.json\n",
            "   ✓ Đã load 2664 cặp từ file có sẵn\n",
            "2. Tạo vector pairs và lưu vào file...\n",
            "   ✓ Đã lưu vector pairs vào /content/vector_pairs.json\n",
            "   ✓ Query: 'Tình cảm anh em'\n",
            "   ✓ Document 1 (relevance: 0): 'Nhiễu điều phủ lấy giá gương Chạy xe nhường nhịn, là thương chính mình...'\n",
            "   ✓ Document 2 (relevance: 0): 'Anh ngồi tựa mạn thuyền rồng Thấy em cuốc cỏ trên đồng anh thương....'\n",
            "   ✓ Label: 0.5 (1.0: doc1 > doc2, 0.0: doc1 < doc2, 0.5: equal)\n",
            "   ✓ Vector size: 768\n",
            "\n",
            "3. Đọc vector pairs từ file và kiểm tra...\n",
            "   ✓ Đã load vectors từ file\n",
            "   ✓ Query: 'Tình cảm anh em'\n",
            "   ✓ Document 1 relevance: 0\n",
            "   ✓ Document 2 relevance: 0\n",
            "   ✓ Ground truth label: 0.5\n",
            "   ✓ Vector shape: (768,)\n",
            "   ✓ Cosine similarity giữa di và dj: 0.6428\n",
            "\n",
            "4. Khởi tạo và load mô hình RankNet...\n",
            "   ✓ Input size: 768\n",
            "   ✓ Detected architecture: hidden_size1=256, hidden_size2=128\n",
            "   ✓ Đã load mô hình từ /content/ranknet_model.pth\n",
            "\n",
            "5. Test mô hình RankNet...\n",
            "   ✓ Score document i: -0.5849\n",
            "   ✓ Score document j: -0.5869\n",
            "   ✓ Score difference (i - j): 0.0020\n",
            "   ✓ P(document i > document j): 0.5005 (50.1%)\n",
            "\n",
            "   📊 ĐÁNH GIÁ KẾT QUẢ:\n",
            "   ✓ Ground truth: Documents i and j should have similar ranks\n",
            "   ✓ Model prediction: Document i có 50.1% khả năng rank cao hơn document j\n",
            "   ✓ Prediction ĐÚNG: ✓\n",
            "\n",
            "   📈 PHÂN TÍCH CHI TIẾT:\n",
            "   ✓ Document 1 relevance: 0 -> Score: -0.5849\n",
            "   ✓ Document 2 relevance: 0 -> Score: -0.5869\n",
            "   ✓ Based on relevance: Documents should rank equally\n",
            "   ✓ Based on model: Document 1 ranks higher\n",
            "\n",
            "6. Test với nhiều cặp khác...\n",
            "   Pair 1: P(doc1 > doc2) = 0.501, Label = 0.5, ✓\n",
            "   Pair 2: P(doc1 > doc2) = 0.499, Label = 0.5, ✓\n",
            "   Pair 3: P(doc1 > doc2) = 0.502, Label = 0.5, ✓\n",
            "   Pair 4: P(doc1 > doc2) = 0.457, Label = 0.5, ✓\n",
            "   Pair 5: P(doc1 > doc2) = 0.503, Label = 0.5, ✓\n",
            "\n",
            "   📊 Accuracy trên 5 cặp test: 1.00 (100.0%)\n",
            "\n",
            "============================================================\n",
            "HOÀN THÀNH KIỂM THỬ MÔ HÌNH RANKNET!\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}