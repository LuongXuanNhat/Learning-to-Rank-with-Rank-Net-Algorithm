{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi -L"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6h7KPFJqNMB",
        "outputId": "31cc84a9-e311-4b75-8e52-427a1408bb6b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-ffb9050d-a787-d844-1573-4371126d4b71)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import c√°c th∆∞ vi·ªán c·∫ßn s·ª≠ d·ª•ng"
      ],
      "metadata": {
        "id": "yLS4WrSarXBi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pyvi\n",
        "# !pip install sentence_transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u-GwQPsx2mEg",
        "outputId": "1c6e6feb-308b-4a91-84b2-aba2d1424753"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pyvi in /usr/local/lib/python3.11/dist-packages (0.1.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (from pyvi) (1.6.1)\n",
            "Requirement already satisfied: sklearn-crfsuite in /usr/local/lib/python3.11/dist-packages (from pyvi) (0.5.0)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn->pyvi) (3.6.0)\n",
            "Requirement already satisfied: python-crfsuite>=0.9.7 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.11)\n",
            "Requirement already satisfied: tabulate>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (0.9.0)\n",
            "Requirement already satisfied: tqdm>=2.0 in /usr/local/lib/python3.11/dist-packages (from sklearn-crfsuite->pyvi) (4.67.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import math\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import re\n",
        "from collections import Counter, defaultdict\n",
        "import os\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from pyvi import ViTokenizer\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "baNRYvdKrrKf"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ƒê·ªãnh nghƒ©a class RankNet - k·∫øt th·ª´a t·ª´ th∆∞ vi·ªán: torch.nn.Module"
      ],
      "metadata": {
        "id": "i4FErFmyv87U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class RankNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size1=256, hidden_size2=128, dropout=0.2):\n",
        "        \"\"\"\n",
        "        RankNet v·ªõi 2 t·∫ßng ·∫©n\n",
        "\n",
        "        Args:\n",
        "            input_size (int): S·ªë features ƒë·∫ßu v√†o\n",
        "            hidden_size1 (int): S·ªë neurons t·∫ßng ·∫©n th·ª© nh·∫•t\n",
        "            hidden_size2 (int): S·ªë neurons t·∫ßng ·∫©n th·ª© hai\n",
        "            dropout (float): T·ª∑ l·ªá dropout ƒë·ªÉ tr√°nh overfitting\n",
        "        \"\"\"\n",
        "        super(RankNet, self).__init__()\n",
        "\n",
        "        # ƒê·ªãnh nghƒ©a c√°c t·∫ßng\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size1)\n",
        "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
        "        self.fc3 = nn.Linear(hidden_size2, 1)  # Output layer cho score\n",
        "\n",
        "        # Dropout layers\n",
        "        self.dropout1 = nn.Dropout(dropout)\n",
        "        self.dropout2 = nn.Dropout(dropout)\n",
        "\n",
        "        # Layer normalization thay v√¨ batch normalization ƒë·ªÉ tr√°nh l·ªói khi batch_size=1\n",
        "        self.ln1 = nn.LayerNorm(hidden_size1)\n",
        "        self.ln2 = nn.LayerNorm(hidden_size2)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Forward pass\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Input tensor c√≥ shape (batch_size, input_size)\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Output scores c√≥ shape (batch_size, 1)\n",
        "        \"\"\"\n",
        "        # ƒê·∫£m b·∫£o input l√† tensor v√† c√≥ ƒë√∫ng ki·ªÉu d·ªØ li·ªáu\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = torch.tensor(x, dtype=torch.float32)\n",
        "\n",
        "        # T·∫ßng ·∫©n th·ª© nh·∫•t\n",
        "        x = self.fc1(x)\n",
        "        x = self.ln1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout1(x)\n",
        "\n",
        "        # T·∫ßng ·∫©n th·ª© hai\n",
        "        x = self.fc2(x)\n",
        "        x = self.ln2(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dropout2(x)\n",
        "\n",
        "        # Output layer\n",
        "        x = self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "    def predict_rank(self, x1, x2):\n",
        "        \"\"\"\n",
        "        So s√°nh ranking gi·ªØa hai samples\n",
        "\n",
        "        Args:\n",
        "            x1, x2 (torch.Tensor): Hai samples c·∫ßn so s√°nh\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: X√°c su·∫•t x1 ƒë∆∞·ª£c rank cao h∆°n x2\n",
        "        \"\"\"\n",
        "        score1 = self.forward(x1)\n",
        "        score2 = self.forward(x2)\n",
        "\n",
        "        # S·ª≠ d·ª•ng sigmoid ƒë·ªÉ chuy·ªÉn v·ªÅ x√°c su·∫•t\n",
        "        prob = torch.sigmoid(score1 - score2)\n",
        "        return prob\n"
      ],
      "metadata": {
        "id": "OPdkzRkzwIcW"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "H√†m m·∫•t m√°t"
      ],
      "metadata": {
        "id": "Zkp0aCK-w58i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def ranknet_loss(s_i, s_j, P_ij):\n",
        "    diff = s_i - s_j\n",
        "    P_hat = torch.sigmoid(diff)  # X√°c su·∫•t d·ª± ƒëo√°n PÃÇ·µ¢‚±º\n",
        "    # Th√™m epsilon ƒë·ªÉ tr√°nh log(0)\n",
        "    epsilon = 1e-10\n",
        "    loss = -P_ij * torch.log(P_hat + epsilon) - (1 - P_ij) * torch.log(1 - P_hat + epsilon)\n",
        "    return loss.mean()"
      ],
      "metadata": {
        "id": "D3YdLQ86w77_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "M·ªôt s·ªë l·ªõp, h√†m ph·ª• tr·ª£"
      ],
      "metadata": {
        "id": "c4zlhnATyWha"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPreparator:\n",
        "    \"\"\"\n",
        "    Class chuy√™n d·ª•ng cho vi·ªác chu·∫©n b·ªã d·ªØ li·ªáu pairwise training\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, sentence_model: SentenceTransformer, stopwords_path: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Kh·ªüi t·∫°o DataPreparator\n",
        "\n",
        "        Args:\n",
        "            sentence_model: M√¥ h√¨nh sentence transformer\n",
        "            stopwords_path: ƒê∆∞·ªùng d·∫´n file stopwords (optional)\n",
        "        \"\"\"\n",
        "        self.sentence_model = sentence_model\n",
        "        self.stopwords = set()\n",
        "\n",
        "        # Load stopwords n·∫øu c√≥\n",
        "        if stopwords_path and os.path.exists(stopwords_path):\n",
        "            with open(stopwords_path, 'r', encoding='utf-8') as f:\n",
        "                self.stopwords = set(line.strip() for line in f if line.strip())\n",
        "            print(f\"ƒê√£ load {len(self.stopwords)} stopwords t·ª´ {stopwords_path}\")\n",
        "\n",
        "    def preprocess_text(self, text: str) -> str:\n",
        "        \"\"\"\n",
        "        Ti·ªÅn x·ª≠ l√Ω vƒÉn b·∫£n v·ªõi PyVi\n",
        "\n",
        "        Args:\n",
        "            text: VƒÉn b·∫£n c·∫ßn x·ª≠ l√Ω\n",
        "\n",
        "        Returns:\n",
        "            str: VƒÉn b·∫£n ƒë√£ ƒë∆∞·ª£c x·ª≠ l√Ω\n",
        "        \"\"\"\n",
        "        # 1. Chu·∫©n h√≥a c∆° b·∫£n\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'[^\\w\\s]', ' ', text)  # lo·∫°i b·ªè d·∫•u c√¢u\n",
        "        text = re.sub(r'\\d+', ' ', text)      # lo·∫°i b·ªè s·ªë\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()  # b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
        "\n",
        "        # 2. Tokenize v·ªõi PyVi\n",
        "        tokenized = ViTokenizer.tokenize(text)\n",
        "\n",
        "        # 3. T√°ch th√†nh danh s√°ch t·ª´ v√† l·ªçc stopwords\n",
        "        tokens = tokenized.split()\n",
        "        if self.stopwords:\n",
        "            tokens = [token for token in tokens if token not in self.stopwords]\n",
        "\n",
        "        return \" \".join(tokens)\n",
        "\n",
        "\n",
        "    def calculate_relevance_scores(self, query: str, documents: List[Dict],\n",
        "                                relevance_method: str = 'cosine_ranking') -> List[Tuple[int, float, int]]:\n",
        "        \"\"\"\n",
        "        T√≠nh ƒëi·ªÉm relevance cho t·∫•t c·∫£ documents v·ªõi m·ªôt query\n",
        "        \"\"\"\n",
        "        # Preprocess query\n",
        "        processed_query = self.preprocess_text(query)\n",
        "\n",
        "        # Encode query\n",
        "        query_embedding = self.sentence_model.encode([processed_query])\n",
        "\n",
        "        # Preprocess v√† encode t·∫•t c·∫£ documents\n",
        "        doc_texts = []\n",
        "        for doc in documents:\n",
        "            processed_doc = self.preprocess_text(doc['value'])\n",
        "            doc_texts.append(processed_doc)\n",
        "\n",
        "        doc_embeddings = self.sentence_model.encode(doc_texts)\n",
        "\n",
        "        # T√≠nh cosine similarity\n",
        "        similarities = cosine_similarity(query_embedding, doc_embeddings)[0]\n",
        "\n",
        "        # G√°n relevance labels - S·ª¨A CH√çNH T·∫†I ƒê√ÇY\n",
        "        doc_relevances = []\n",
        "\n",
        "        if relevance_method == 'cosine_ranking':\n",
        "            # S·∫Øp x·∫øp theo similarity v√† g√°n labels theo ranking\n",
        "            doc_similarities = [(i, sim) for i, sim in enumerate(similarities)]\n",
        "            doc_similarities.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # Ph√¢n ph·ªëi relevance t·ªët h∆°n\n",
        "            total_docs = len(doc_similarities)\n",
        "            for rank, (doc_idx, similarity) in enumerate(doc_similarities):\n",
        "                if rank < max(1, total_docs // 10):  # Top 10%: Highly relevant\n",
        "                    relevance = 3\n",
        "                elif rank < max(2, total_docs // 5):  # Top 20%: Relevant\n",
        "                    relevance = 2\n",
        "                elif rank < max(3, total_docs // 3):  # Top 33%: Somewhat relevant\n",
        "                    relevance = 1\n",
        "                else:  # C√≤n l·∫°i: Not relevant\n",
        "                    relevance = 0\n",
        "\n",
        "                doc_relevances.append((doc_idx, float(similarity), relevance))\n",
        "\n",
        "        elif relevance_method == 'threshold_based':\n",
        "            # ƒêi·ªÅu ch·ªânh threshold cho ph√π h·ª£p h∆°n\n",
        "            for doc_idx, similarity in enumerate(similarities):\n",
        "                if similarity >= 0.7:  # Gi·∫£m t·ª´ 0.8 xu·ªëng 0.7\n",
        "                    relevance = 3\n",
        "                elif similarity >= 0.5:  # Gi·∫£m t·ª´ 0.6 xu·ªëng 0.5\n",
        "                    relevance = 2\n",
        "                elif similarity >= 0.3:  # Gi·∫£m t·ª´ 0.4 xu·ªëng 0.3\n",
        "                    relevance = 1\n",
        "                else:\n",
        "                    relevance = 0\n",
        "\n",
        "                doc_relevances.append((doc_idx, float(similarity), relevance))\n",
        "\n",
        "        return doc_relevances\n",
        "\n",
        "    def generate_pairwise_data(self, queries: List[str], documents: List[Dict],\n",
        "                             relevance_method: str = 'cosine_ranking',\n",
        "                             max_pairs_per_query: Optional[int] = None,\n",
        "                             balance_labels: bool = True) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        T·∫°o d·ªØ li·ªáu pairwise training\n",
        "\n",
        "        Args:\n",
        "            queries: Danh s√°ch queries\n",
        "            documents: Danh s√°ch documents\n",
        "            relevance_method: Ph∆∞∆°ng ph√°p t√≠nh relevance\n",
        "            max_pairs_per_query: Gi·ªõi h·∫°n s·ªë c·∫∑p per query (optional)\n",
        "            balance_labels: C√≥ c√¢n b·∫±ng labels kh√¥ng\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Danh s√°ch pairwise samples\n",
        "        \"\"\"\n",
        "        print(f\"B·∫Øt ƒë·∫ßu t·∫°o d·ªØ li·ªáu pairwise v·ªõi {len(queries)} queries v√† {len(documents)} documents\")\n",
        "\n",
        "        all_pairwise_data = []\n",
        "        label_counts = {0.0: 0, 0.5: 0, 1.0: 0}\n",
        "\n",
        "        for query_idx, query in enumerate(tqdm(queries, desc=\"T·∫°o d·ªØ li·ªáu pairwise\")):\n",
        "            # T√≠nh relevance scores\n",
        "            doc_relevances = self.calculate_relevance_scores(query, documents, relevance_method)\n",
        "\n",
        "            # T·∫°o t·∫•t c·∫£ c√°c c·∫∑p possible\n",
        "            query_pairs = []\n",
        "\n",
        "            for i in range(len(doc_relevances)):\n",
        "                for j in range(i + 1, len(doc_relevances)):\n",
        "                    doc1_idx, doc1_sim, doc1_rel = doc_relevances[i]\n",
        "                    doc2_idx, doc2_sim, doc2_rel = doc_relevances[j]\n",
        "\n",
        "                    # X√°c ƒë·ªãnh label cho c·∫∑p\n",
        "                    if doc1_rel > doc2_rel:\n",
        "                        label = 1.0  # Doc1 t·ªët h∆°n Doc2\n",
        "                    elif doc1_rel < doc2_rel:\n",
        "                        label = 0.0  # Doc1 k√©m h∆°n Doc2\n",
        "                    else:\n",
        "                        label = 0.5  # B·∫±ng nhau\n",
        "\n",
        "                    # T·∫°o sample\n",
        "                    pairwise_sample = {\n",
        "                        \"query_id\": query_idx,\n",
        "                        \"query\": query,\n",
        "                        \"query_processed\": self.preprocess_text(query),\n",
        "                        \"doc1_id\": documents[doc1_idx]['id'],\n",
        "                        \"doc1_text\": documents[doc1_idx]['value'],\n",
        "                        \"doc1_processed\": self.preprocess_text(documents[doc1_idx]['value']),\n",
        "                        \"doc1_relevance\": doc1_rel,\n",
        "                        \"doc1_similarity\": doc1_sim,\n",
        "                        \"doc2_id\": documents[doc2_idx]['id'],\n",
        "                        \"doc2_text\": documents[doc2_idx]['value'],\n",
        "                        \"doc2_processed\": self.preprocess_text(documents[doc2_idx]['value']),\n",
        "                        \"doc2_relevance\": doc2_rel,\n",
        "                        \"doc2_similarity\": doc2_sim,\n",
        "                        \"label\": label,\n",
        "                        \"relevance_method\": relevance_method\n",
        "                    }\n",
        "\n",
        "                    query_pairs.append(pairwise_sample)\n",
        "                    label_counts[label] += 1\n",
        "\n",
        "            # Gi·ªõi h·∫°n s·ªë c·∫∑p per query n·∫øu c·∫ßn\n",
        "            if max_pairs_per_query and len(query_pairs) > max_pairs_per_query:\n",
        "                # Sampling c√≥ c√¢n b·∫±ng labels\n",
        "                if balance_labels:\n",
        "                    query_pairs = self._balanced_sampling(query_pairs, max_pairs_per_query)\n",
        "                else:\n",
        "                    query_pairs = query_pairs[:max_pairs_per_query]\n",
        "\n",
        "            all_pairwise_data.extend(query_pairs)\n",
        "\n",
        "        # In th·ªëng k√™\n",
        "        total_pairs = len(all_pairwise_data)\n",
        "        print(f\"\\nTh·ªëng k√™ d·ªØ li·ªáu pairwise:\")\n",
        "        print(f\"T·ªïng s·ªë c·∫∑p: {total_pairs}\")\n",
        "        print(f\"Label 0.0 (doc1 < doc2): {label_counts[0.0]} ({label_counts[0.0]/total_pairs*100:.1f}%)\")\n",
        "        print(f\"Label 0.5 (doc1 = doc2): {label_counts[0.5]} ({label_counts[0.5]/total_pairs*100:.1f}%)\")\n",
        "        print(f\"Label 1.0 (doc1 > doc2): {label_counts[1.0]} ({label_counts[1.0]/total_pairs*100:.1f}%)\")\n",
        "\n",
        "        return all_pairwise_data\n",
        "\n",
        "    def _balanced_sampling(self, pairs: List[Dict], max_pairs: int) -> List[Dict]:\n",
        "        \"\"\"\n",
        "        L·∫•y m·∫´u c√¢n b·∫±ng theo labels\n",
        "\n",
        "        Args:\n",
        "            pairs: Danh s√°ch t·∫•t c·∫£ c√°c c·∫∑p\n",
        "            max_pairs: S·ªë c·∫∑p t·ªëi ƒëa\n",
        "\n",
        "        Returns:\n",
        "            List[Dict]: Danh s√°ch c·∫∑p ƒë√£ ƒë∆∞·ª£c sampling\n",
        "        \"\"\"\n",
        "        # Ph√¢n lo·∫°i theo labels\n",
        "        label_groups = {0.0: [], 0.5: [], 1.0: []}\n",
        "        for pair in pairs:\n",
        "            label_groups[pair['label']].append(pair)\n",
        "\n",
        "        # T√≠nh s·ªë m·∫´u cho m·ªói label (c√¢n b·∫±ng)\n",
        "        samples_per_label = max_pairs // 3\n",
        "        remaining = max_pairs % 3\n",
        "\n",
        "        sampled_pairs = []\n",
        "\n",
        "        # L·∫•y m·∫´u t·ª´ng label\n",
        "        for i, (label, group) in enumerate(label_groups.items()):\n",
        "            n_samples = samples_per_label + (1 if i < remaining else 0)\n",
        "            n_samples = min(n_samples, len(group))  # Kh√¥ng v∆∞·ª£t qu√° s·ªë c√≥ s·∫µn\n",
        "\n",
        "            # Random sampling\n",
        "            import random\n",
        "            sampled = random.sample(group, n_samples)\n",
        "            sampled_pairs.extend(sampled)\n",
        "\n",
        "        return sampled_pairs\n",
        "\n",
        "    def save_pairwise_data(self, pairwise_data: List[Dict], file_path: str,\n",
        "                          metadata: Optional[Dict] = None):\n",
        "        \"\"\"\n",
        "        L∆∞u d·ªØ li·ªáu pairwise v√†o file\n",
        "\n",
        "        Args:\n",
        "            pairwise_data: D·ªØ li·ªáu pairwise\n",
        "            file_path: ƒê∆∞·ªùng d·∫´n file\n",
        "            metadata: Metadata b·ªï sung (optional)\n",
        "        \"\"\"\n",
        "        # T·∫°o th∆∞ m·ª•c n·∫øu ch∆∞a c√≥\n",
        "        os.makedirs(os.path.dirname(file_path) if os.path.dirname(file_path) else '.', exist_ok=True)\n",
        "\n",
        "        # Chu·∫©n b·ªã data ƒë·ªÉ l∆∞u\n",
        "        data_to_save = {\n",
        "            \"metadata\": {\n",
        "                \"total_pairs\": len(pairwise_data),\n",
        "                \"created_by\": \"DataPreparator\",\n",
        "                \"format_version\": \"1.0\",\n",
        "                **(metadata or {})\n",
        "            },\n",
        "            \"pairwise_data\": pairwise_data\n",
        "        }\n",
        "\n",
        "        # L∆∞u file\n",
        "        with open(file_path, 'w', encoding='utf-8') as f:\n",
        "            json.dump(data_to_save, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"ƒê√£ l∆∞u {len(pairwise_data)} c·∫∑p d·ªØ li·ªáu v√†o {file_path}\")\n",
        "\n",
        "    def load_pairwise_data(self, file_path: str) -> Tuple[List[Dict], Dict]:\n",
        "        \"\"\"\n",
        "        Load d·ªØ li·ªáu pairwise t·ª´ file\n",
        "\n",
        "        Args:\n",
        "            file_path: ƒê∆∞·ªùng d·∫´n file\n",
        "\n",
        "        Returns:\n",
        "            Tuple[List[Dict], Dict]: (pairwise_data, metadata)\n",
        "        \"\"\"\n",
        "        if not os.path.exists(file_path):\n",
        "            raise FileNotFoundError(f\"File kh√¥ng t·ªìn t·∫°i: {file_path}\")\n",
        "\n",
        "        with open(file_path, 'r', encoding='utf-8') as f:\n",
        "            data = json.load(f)\n",
        "\n",
        "        pairwise_data = data.get('pairwise_data', [])\n",
        "        metadata = data.get('metadata', {})\n",
        "\n",
        "        print(f\"ƒê√£ load {len(pairwise_data)} c·∫∑p d·ªØ li·ªáu t·ª´ {file_path}\")\n",
        "\n",
        "        return pairwise_data, metadata\n",
        "def prepare_training_data(documents_path: str, queries: List[str],\n",
        "                         output_file: str = \"document_pairs.json\",\n",
        "                         sentence_model_name: str = 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',\n",
        "                         stopwords_path: Optional[str] = None,\n",
        "                         relevance_method: str = 'cosine_ranking',\n",
        "                         max_pairs_per_query: Optional[int] = None):\n",
        "    \"\"\"\n",
        "    H√†m wrapper ƒë·ªÉ chu·∫©n b·ªã d·ªØ li·ªáu training m·ªôt c√°ch d·ªÖ d√†ng\n",
        "\n",
        "    Args:\n",
        "        documents_path: ƒê∆∞·ªùng d·∫´n file documents JSON\n",
        "        queries: Danh s√°ch queries\n",
        "        output_file: T√™n file output\n",
        "        sentence_model_name: T√™n sentence transformer model\n",
        "        stopwords_path: ƒê∆∞·ªùng d·∫´n stopwords file\n",
        "        relevance_method: Ph∆∞∆°ng ph√°p t√≠nh relevance\n",
        "        max_pairs_per_query: Gi·ªõi h·∫°n c·∫∑p per query\n",
        "\n",
        "    Returns:\n",
        "        List[Dict]: D·ªØ li·ªáu pairwise ƒë√£ t·∫°o\n",
        "    \"\"\"\n",
        "    print(\"=\"*60)\n",
        "    print(\"CHU·∫®N B·ªä D·ªÆ LI·ªÜU PAIRWISE CHO RANKNET\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    # 1. Load documents\n",
        "    print(f\"1. ƒêang load documents t·ª´ {documents_path}...\")\n",
        "    with open(documents_path, 'r', encoding='utf-8') as f:\n",
        "        documents = json.load(f)\n",
        "    print(f\"   ‚úì ƒê√£ load {len(documents)} documents\")\n",
        "\n",
        "    # 2. Kh·ªüi t·∫°o sentence transformer\n",
        "    print(f\"2. ƒêang kh·ªüi t·∫°o sentence transformer: {sentence_model_name}...\")\n",
        "    sentence_model = SentenceTransformer(sentence_model_name)\n",
        "    print(f\"   ‚úì Model ƒë√£ s·∫µn s√†ng\")\n",
        "\n",
        "    # 3. Kh·ªüi t·∫°o DataPreparator\n",
        "    print(\"3. ƒêang kh·ªüi t·∫°o DataPreparator...\")\n",
        "    preparator = DataPreparator(sentence_model, stopwords_path)\n",
        "    print(f\"   ‚úì DataPreparator ƒë√£ s·∫µn s√†ng\")\n",
        "\n",
        "    # 4. T·∫°o pairwise data\n",
        "    print(\"4. ƒêang t·∫°o d·ªØ li·ªáu pairwise...\")\n",
        "    pairwise_data = preparator.generate_pairwise_data(\n",
        "        queries=queries,\n",
        "        documents=documents,\n",
        "        relevance_method=relevance_method,\n",
        "        max_pairs_per_query=max_pairs_per_query,\n",
        "        balance_labels=True\n",
        "    )\n",
        "    print(f\"   ‚úì ƒê√£ t·∫°o {len(pairwise_data)} c·∫∑p d·ªØ li·ªáu\")\n",
        "\n",
        "    # 5. L∆∞u file\n",
        "    print(f\"5. ƒêang l∆∞u v√†o {output_file}...\")\n",
        "    metadata = {\n",
        "        \"documents_path\": documents_path,\n",
        "        \"num_queries\": len(queries),\n",
        "        \"queries\": queries,\n",
        "        \"relevance_method\": relevance_method,\n",
        "        \"sentence_model\": sentence_model_name,\n",
        "        \"max_pairs_per_query\": max_pairs_per_query\n",
        "    }\n",
        "\n",
        "    preparator.save_pairwise_data(pairwise_data, output_file, metadata)\n",
        "    print(f\"   ‚úì D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng\")\n",
        "\n",
        "    print(\"=\"*60)\n",
        "    print(\"HO√ÄN TH√ÄNH CHU·∫®N B·ªä D·ªÆ LI·ªÜU!\")\n",
        "    print(\"=\"*60)\n",
        "\n",
        "    return pairwise_data"
      ],
      "metadata": {
        "id": "8Q-8K8nHKJvX"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Chu·∫©n b·ªã d·ªØ li·ªáu, l·∫•y ra so s√°nh\n",
        "class RankNetDataset(Dataset):\n",
        "    \"\"\"Dataset cho RankNet training\"\"\"\n",
        "    def __init__(self, pairwise_data, sentence_model):\n",
        "        self.data = pairwise_data\n",
        "        self.sentence_model = sentence_model\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    #  Tr·∫£ v·ªÅ m·ªôt dictionary ch·ª©a feature1, feature2, v√† label\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.data[idx]\n",
        "\n",
        "        # Encode query v√† documents\n",
        "        query_emb = self.sentence_model.encode([sample['query']])[0]\n",
        "        doc1_emb = self.sentence_model.encode([sample['doc1_text']])[0]\n",
        "        doc2_emb = self.sentence_model.encode([sample['doc2_text']])[0]\n",
        "\n",
        "        # T·∫°o features b·∫±ng c√°ch concat query+doc\n",
        "        feature1 = np.concatenate([query_emb, doc1_emb])\n",
        "        feature2 = np.concatenate([query_emb, doc2_emb])\n",
        "\n",
        "        return {\n",
        "            'feature1': torch.tensor(feature1, dtype=torch.float32),\n",
        "            'feature2': torch.tensor(feature2, dtype=torch.float32),\n",
        "            'label': torch.tensor(sample['label'], dtype=torch.float32)\n",
        "        }\n",
        "\n",
        "\n",
        "def evaluate_ranking(model, documents, queries, sentence_model, preparator, device, top_k=10):\n",
        "    \"\"\"\n",
        "    ƒê√°nh gi√° m√¥ h√¨nh b·∫±ng c√°ch rank l·∫°i documents cho m·ªói query\n",
        "    v√† t√≠nh pairwise accuracy\n",
        "    \"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    results = {}\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for query in queries:\n",
        "            # S·ª¨A: G·ªçi ƒë√∫ng method preprocess_text\n",
        "            processed_query = preparator.preprocess_text(query)\n",
        "\n",
        "            # Encode query\n",
        "            query_emb = sentence_model.encode([processed_query])[0]\n",
        "\n",
        "            # Score t·∫•t c·∫£ documents\n",
        "            doc_scores = []\n",
        "            for doc in documents:\n",
        "                # S·ª¨A: G·ªçi ƒë√∫ng method preprocess_text\n",
        "                processed_doc = preparator.preprocess_text(doc['value'])\n",
        "\n",
        "                doc_emb = sentence_model.encode([processed_doc])[0]\n",
        "\n",
        "                feature = torch.tensor(\n",
        "                    np.concatenate([query_emb, doc_emb]),\n",
        "                    dtype=torch.float32\n",
        "                ).unsqueeze(0).to(device)\n",
        "\n",
        "                score = model(feature).item()\n",
        "                doc_scores.append((doc['id'], score, doc))\n",
        "\n",
        "            # S·∫Øp x·∫øp gi·∫£m d·∫ßn theo score\n",
        "            doc_scores.sort(key=lambda x: x[1], reverse=True)\n",
        "\n",
        "            # L·∫•y top k\n",
        "            top_docs = doc_scores[:top_k]\n",
        "\n",
        "            results[query] = {\n",
        "                'top_documents': [(doc_id, doc['value'], score) for doc_id, score, doc in top_docs],\n",
        "                'all_scores': doc_scores\n",
        "            }\n",
        "\n",
        "    return results\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "QpI7PnRkybGn"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# H√†m hu·∫•n luy·ªán\n",
        "def train_ranknet(model, dataloader, optimizer, device, num_epochs=20,\n",
        "                          loss_type='cross_entropy', patience=5, min_delta=1e-4):\n",
        "    \"\"\"\n",
        "    Hu·∫•n luy·ªán m√¥ h√¨nh RankNet v·ªõi c√°c c·∫£i ti·∫øn\n",
        "\n",
        "    Args:\n",
        "        model: RankNet model\n",
        "        dataloader: DataLoader\n",
        "        optimizer: optimizer\n",
        "        device: cuda/cpu\n",
        "        num_epochs: s·ªë epochs\n",
        "        loss_type: lo·∫°i loss function\n",
        "        patience: s·ªë epochs ch·ªù ƒë·ªÉ early stopping\n",
        "        min_delta: threshold nh·ªè nh·∫•t ƒë·ªÉ coi l√† improvement\n",
        "    \"\"\"\n",
        "    model.train()\n",
        "    model.to(device)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=3, verbose=True, min_lr=1e-6\n",
        "    )\n",
        "\n",
        "    # Early stopping variables\n",
        "    best_loss = float('inf')\n",
        "    patience_counter = 0\n",
        "    loss_history = []\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        total_loss = 0\n",
        "        num_batches = 0\n",
        "\n",
        "        progress_bar = tqdm(dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "\n",
        "        for batch in progress_bar:\n",
        "            feature1 = batch['feature1'].to(device)\n",
        "            feature2 = batch['feature2'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            score1 = model(feature1).squeeze()\n",
        "            score2 = model(feature2).squeeze()\n",
        "\n",
        "            # T√≠nh loss v·ªõi h√†m c·∫£i ti·∫øn\n",
        "            loss = ranknet_loss(score1, score2, labels)\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping ƒë·ªÉ tr√°nh exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            total_loss += loss.item()\n",
        "            num_batches += 1\n",
        "\n",
        "            progress_bar.set_postfix({\n",
        "                'Loss': f\"{loss.item():.4f}\",\n",
        "                'LR': f\"{optimizer.param_groups[0]['lr']:.6f}\"\n",
        "            })\n",
        "\n",
        "        avg_loss = total_loss / num_batches\n",
        "        loss_history.append(avg_loss)\n",
        "\n",
        "        # Learning rate scheduling\n",
        "        scheduler.step(avg_loss)\n",
        "\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}, Average Loss: {avg_loss:.4f}\")\n",
        "\n",
        "        # Early stopping check\n",
        "        if avg_loss < best_loss - min_delta:\n",
        "            best_loss = avg_loss\n",
        "            patience_counter = 0\n",
        "            # Save best model\n",
        "            torch.save(model.state_dict(), 'best_ranknet_model.pth')\n",
        "            print(f\"New best model saved with loss: {best_loss:.4f}\")\n",
        "        else:\n",
        "            patience_counter += 1\n",
        "\n",
        "        if patience_counter >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            print(f\"Loading best model with loss: {best_loss:.4f}\")\n",
        "            model.load_state_dict(torch.load('best_ranknet_model.pth'))\n",
        "            break\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "KcoSWYiDK4IR"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# H√†m ƒë√°nh gi√° k·∫øt qu·∫£\n",
        "def calculate_pairwise_accuracy(model, test_data, sentence_model, device):\n",
        "    \"\"\"T√≠nh Pairwise Accuracy\"\"\"\n",
        "    model.eval()\n",
        "    model.to(device)\n",
        "\n",
        "    correct_predictions = 0\n",
        "    total_predictions = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for sample in tqdm(test_data, desc=\"T√≠nh Pairwise Accuracy\"):\n",
        "            # Encode features\n",
        "            query_emb = sentence_model.encode([sample['query']])[0]\n",
        "            doc1_emb = sentence_model.encode([sample['doc1_text']])[0]\n",
        "            doc2_emb = sentence_model.encode([sample['doc2_text']])[0]\n",
        "\n",
        "            feature1 = torch.tensor(np.concatenate([query_emb, doc1_emb]),\n",
        "                                  dtype=torch.float32).unsqueeze(0).to(device)\n",
        "            feature2 = torch.tensor(np.concatenate([query_emb, doc2_emb]),\n",
        "                                  dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "            # T√≠nh scores\n",
        "            score1 = model(feature1).item()\n",
        "            score2 = model(feature2).item()\n",
        "\n",
        "            # D·ª± ƒëo√°n\n",
        "            if sample['label'] == 1.0:  # doc1 should be ranked higher\n",
        "                if score1 > score2:\n",
        "                    correct_predictions += 1\n",
        "            elif sample['label'] == 0.0:  # doc2 should be ranked higher\n",
        "                if score2 > score1:\n",
        "                    correct_predictions += 1\n",
        "            else:  # Equal relevance (label = 0.5)\n",
        "                # Coi nh∆∞ ƒë√∫ng n·∫øu difference nh·ªè\n",
        "                if abs(score1 - score2) < 0.1:\n",
        "                    correct_predictions += 1\n",
        "\n",
        "            total_predictions += 1\n",
        "\n",
        "    accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "    return accuracy\n",
        "\n",
        "  # H√†m test\n",
        "def test_specific_document_pair(model, sentence_model, preparator, documents,\n",
        "                              query, doc_id1, doc_id2, device):\n",
        "  \"\"\"\n",
        "  Test m√¥ h√¨nh v·ªõi c·∫∑p t√†i li·ªáu c·ª• th·ªÉ theo ID\n",
        "\n",
        "  Args:\n",
        "      model: RankNet model ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán\n",
        "      sentence_model: SentenceTransformer model\n",
        "      preparator: DataPreparator\n",
        "      documents: List t·∫•t c·∫£ documents\n",
        "      query: Query string ƒë·ªÉ test\n",
        "      doc_id1, doc_id2: ID c·ªßa 2 documents c·∫ßn so s√°nh\n",
        "      device: cuda/cpu\n",
        "  \"\"\"\n",
        "  print(f\"=\"*60)\n",
        "  print(f\"TEST C·∫∂P T√ÄI LI·ªÜU C·ª§ TH·ªÇ\")\n",
        "  print(f\"=\"*60)\n",
        "\n",
        "  # T√¨m documents theo ID\n",
        "  doc1 = None\n",
        "  doc2 = None\n",
        "\n",
        "  for doc in documents:\n",
        "      if doc['id'] == doc_id1:\n",
        "          doc1 = doc\n",
        "      elif doc['id'] == doc_id2:\n",
        "          doc2 = doc\n",
        "\n",
        "  if doc1 is None:\n",
        "      print(f\"‚ùå Kh√¥ng t√¨m th·∫•y document v·ªõi ID: {doc_id1}\")\n",
        "      return\n",
        "  if doc2 is None:\n",
        "      print(f\"‚ùå Kh√¥ng t√¨m th·∫•y document v·ªõi ID: {doc_id2}\")\n",
        "      return\n",
        "\n",
        "  print(f\"üìÑ Query: '{query}'\")\n",
        "  print(f\"üìÑ Document 1 [ID: {doc_id1}]: '{doc1['value']}'\")\n",
        "  print(f\"üìÑ Document 2 [ID: {doc_id2}]: '{doc2['value']}'\")\n",
        "  print()\n",
        "\n",
        "  # Preprocess v√† encode\n",
        "  processed_query = preparator.preprocess_text(query)\n",
        "  processed_doc1 = preparator.preprocess_text(doc1['value'])\n",
        "  processed_doc2 = preparator.preprocess_text(doc2['value'])\n",
        "\n",
        "  query_emb = sentence_model.encode([processed_query])[0]\n",
        "  doc1_emb = sentence_model.encode([processed_doc1])[0]\n",
        "  doc2_emb = sentence_model.encode([processed_doc2])[0]\n",
        "\n",
        "  # T·∫°o features (concatenation)\n",
        "  feature1 = np.concatenate([query_emb, doc1_emb])\n",
        "  feature2 = np.concatenate([query_emb, doc2_emb])\n",
        "\n",
        "  print(f\"üîß Preprocessed query: '{processed_query}'\")\n",
        "  print(f\"üîß Preprocessed doc1: '{processed_doc1}'\")\n",
        "  print(f\"üîß Preprocessed doc2: '{processed_doc2}'\")\n",
        "  print(f\"üîß Feature vector size: {len(feature1)}\")\n",
        "  print()\n",
        "\n",
        "  # Test v·ªõi model\n",
        "  model.eval()\n",
        "  with torch.no_grad():\n",
        "      # Chuy·ªÉn th√†nh tensor\n",
        "      feature1_tensor = torch.tensor(feature1, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "      feature2_tensor = torch.tensor(feature2, dtype=torch.float32).unsqueeze(0).to(device)\n",
        "\n",
        "      # T√≠nh scores\n",
        "      score1 = model(feature1_tensor).item()\n",
        "      score2 = model(feature2_tensor).item()\n",
        "\n",
        "      # T√≠nh x√°c su·∫•t\n",
        "      diff = score1 - score2\n",
        "      probability_1_better_2 = torch.sigmoid(torch.tensor(diff)).item()\n",
        "\n",
        "      print(f\"üìä K·∫æT QU·∫¢:\")\n",
        "      print(f\"   Score Document 1: {score1:.4f}\")\n",
        "      print(f\"   Score Document 2: {score2:.4f}\")\n",
        "      print(f\"   Score difference: {diff:.4f}\")\n",
        "      print(f\"   P(Doc1 > Doc2): {probability_1_better_2:.4f} ({probability_1_better_2*100:.1f}%)\")\n",
        "      print()\n",
        "\n",
        "      # K·∫øt lu·∫≠n\n",
        "      if probability_1_better_2 > 0.6:\n",
        "          conclusion = f\"Document 1 (ID: {doc_id1}) c√≥ kh·∫£ nƒÉng cao h∆°n li√™n quan v·ªõi query\"\n",
        "      elif probability_1_better_2 < 0.4:\n",
        "          conclusion = f\"Document 2 (ID: {doc_id2}) c√≥ kh·∫£ nƒÉng cao h∆°n li√™n quan v·ªõi query\"\n",
        "      else:\n",
        "          conclusion = \"Hai documents c√≥ m·ª©c ƒë·ªô li√™n quan t∆∞∆°ng ƒë∆∞∆°ng\"\n",
        "\n",
        "      print(f\"üéØ K·∫æT LU·∫¨N: {conclusion}\")\n",
        "\n",
        "      # T√≠nh cosine similarity ƒë·ªÉ so s√°nh\n",
        "      from sklearn.metrics.pairwise import cosine_similarity\n",
        "      query_doc1_sim = cosine_similarity([query_emb], [doc1_emb])[0][0]\n",
        "      query_doc2_sim = cosine_similarity([query_emb], [doc2_emb])[0][0]\n",
        "\n",
        "      print(f\"\\nüìà SO S√ÅNH V·ªöI COSINE SIMILARITY:\")\n",
        "      print(f\"   Query vs Doc1: {query_doc1_sim:.4f}\")\n",
        "      print(f\"   Query vs Doc2: {query_doc2_sim:.4f}\")\n",
        "\n",
        "      if query_doc1_sim > query_doc2_sim:\n",
        "          cosine_conclusion = f\"Theo cosine similarity: Document 1 li√™n quan h∆°n\"\n",
        "      elif query_doc2_sim > query_doc1_sim:\n",
        "          cosine_conclusion = f\"Theo cosine similarity: Document 2 li√™n quan h∆°n\"\n",
        "      else:\n",
        "          cosine_conclusion = f\"Theo cosine similarity: Hai documents t∆∞∆°ng ƒë∆∞∆°ng\"\n",
        "\n",
        "      print(f\"   {cosine_conclusion}\")\n",
        "\n",
        "      # So s√°nh k·∫øt qu·∫£\n",
        "      model_prefers_doc1 = probability_1_better_2 > 0.5\n",
        "      cosine_prefers_doc1 = query_doc1_sim > query_doc2_sim\n",
        "\n",
        "      if model_prefers_doc1 == cosine_prefers_doc1:\n",
        "          print(f\"   ‚úÖ Model v√† Cosine similarity ƒë·ªìng √Ω v·ªõi nhau\")\n",
        "      else:\n",
        "          print(f\"   ‚ö†Ô∏è  Model v√† Cosine similarity c√≥ k·∫øt qu·∫£ kh√°c nhau\")\n",
        "\n",
        "  print(f\"=\"*60)"
      ],
      "metadata": {
        "id": "CFhp1xa9MT7Y"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Ch·∫°y ch∆∞∆°ng tr√¨nh** T√≠nh to√°n ch√≠nh"
      ],
      "metadata": {
        "id": "nX9zBcDTydNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        # Thi·∫øt l·∫≠p device\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"S·ª≠ d·ª•ng device: {device}\")\n",
        "\n",
        "        # B∆∞·ªõc 1: T·∫£i t√†i li·ªáu v√† kh·ªüi t·∫°o sentence-transformer\n",
        "        documents_path = \"/content/cadao_tucngu_50_1.json\"\n",
        "        pairwise_data_file = \"/content/document_pairs.json\"\n",
        "        # documents_path = \"FetchData/train/anh_em_mot_nha/data.json\"\n",
        "\n",
        "        # Ch·∫°y document ƒë·ªÉ l·∫•y ƒë√°nh gi√° v·ªõi k·∫øt qu·∫£\n",
        "        with open(documents_path, 'r', encoding='utf-8') as f:\n",
        "            documents = json.load(f)\n",
        "        stopwords_path = \"/content/vietnamese-stopwords.txt\"\n",
        "        # T·∫°o c·∫∑p d·ªØ li·ªáu hu·∫•n luy·ªán - C·∫£i thi·ªán v·ªõi multiple queries\n",
        "        queries = [\n",
        "            \"Anh em nh∆∞ th·ªÉ tay ch√¢n\"\n",
        "        ]\n",
        "\n",
        "        if os.path.exists(pairwise_data_file):\n",
        "            print(f\"1. Load d·ªØ li·ªáu pairwise t·ª´ {pairwise_data_file}...\")\n",
        "\n",
        "            # Kh·ªüi t·∫°o sentence model tr∆∞·ªõc\n",
        "            sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "            preparator = DataPreparator(sentence_model)\n",
        "\n",
        "            # Load d·ªØ li·ªáu\n",
        "            pairwise_data, metadata = preparator.load_pairwise_data(pairwise_data_file)\n",
        "            print(f\"   ‚úì ƒê√£ load {len(pairwise_data)} c·∫∑p t·ª´ file c√≥ s·∫µn\")\n",
        "            print(f\"   ‚úì Metadata: {metadata}\")\n",
        "\n",
        "        else:\n",
        "            print(\"1. T·∫°o d·ªØ li·ªáu pairwise m·ªõi...\")\n",
        "\n",
        "            # T·∫°o d·ªØ li·ªáu pairwise\n",
        "            pairwise_data = prepare_training_data(\n",
        "                documents_path=documents_path,\n",
        "                queries=queries,\n",
        "                output_file=pairwise_data_file,\n",
        "                stopwords_path=None,\n",
        "                relevance_method=\"cosine_ranking\",\n",
        "                max_pairs_per_query=1000\n",
        "            )\n",
        "\n",
        "            # Kh·ªüi t·∫°o sentence model\n",
        "            sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2') #384 neurons\n",
        "        # T·∫°o dataset v√† dataloader\n",
        "        print(\"ƒêang chu·∫©n b·ªã dataset...\")\n",
        "        dataset = RankNetDataset(pairwise_data, sentence_model)\n",
        "        dataloader = DataLoader(dataset, batch_size=32, shuffle=True, num_workers=0)\n",
        "\n",
        "        # B∆∞·ªõc 2: Kh·ªüi t·∫°o v√† hu·∫•n luy·ªán m√¥ h√¨nh RankNet\n",
        "        print(\"ƒêang kh·ªüi t·∫°o m√¥ h√¨nh RankNet...\")\n",
        "\n",
        "        # T√≠nh k√≠ch th∆∞·ªõc input (query embedding + document embedding)\n",
        "        sample_query_emb = sentence_model.encode([\"test\"])[0]\n",
        "        input_size = len(sample_query_emb) * 2  # query + document embeddings\n",
        "        print(f\"Input size: {input_size}\")\n",
        "\n",
        "        model = RankNet(input_size=input_size, hidden_size1=128, hidden_size2=64, dropout=0.5)\n",
        "        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "\n",
        "        print(\"B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh...\")\n",
        "        model = train_ranknet(model, dataloader, optimizer, device, num_epochs=10)\n",
        "\n",
        "        # L∆∞u m√¥ h√¨nh\n",
        "        torch.save(model.state_dict(), 'ranknet_model.pth')\n",
        "        print(\"ƒê√£ l∆∞u m√¥ h√¨nh v√†o ranknet_model.pth\")\n",
        "\n",
        "                # B∆∞·ªõc 3: ƒê√°nh gi√° ƒë·ªô ch√≠nh x√°c c·ªßa m√¥ h√¨nh\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"ƒê√ÅNH GI√Å M√î H√åNH\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # Test queries\n",
        "        test_queries = [\n",
        "            \"Anh em nh∆∞ th·ªÉ tay ch√¢n\",\n",
        "        ]\n",
        "\n",
        "        print(\"ƒêang ƒë√°nh gi√° ranking...\")\n",
        "        ranking_results = evaluate_ranking(model, documents, test_queries, sentence_model, preparator, device, top_k=10)\n",
        "\n",
        "\n",
        "        # Hi·ªÉn th·ªã k·∫øt qu·∫£ top 10 cho m·ªói query\n",
        "        for query, result in ranking_results.items():\n",
        "            print(f\"\\nQuery: '{query}'\")\n",
        "            print(\"Top 10 documents:\")\n",
        "            for i, (doc_id, doc_text, score) in enumerate(result['top_documents'], 1):\n",
        "                print(f\"{i:2d}. [ID: {doc_id}] Score: {score:.4f}\")\n",
        "                print(f\"    Text: {doc_text[:100]}...\")\n",
        "                print()\n",
        "\n",
        "        # T√≠nh Pairwise Accuracy tr√™n t·∫≠p test\n",
        "        print(\"ƒêang t√≠nh Pairwise Accuracy...\")\n",
        "\n",
        "        # T·∫°o test data t·ª´ test queries\n",
        "        test_pairwise_data = prepare_training_data(\n",
        "                documents_path=documents_path,\n",
        "                queries=queries,\n",
        "                output_file=pairwise_data_file,\n",
        "                stopwords_path=None,\n",
        "                relevance_method=\"cosine_ranking\",\n",
        "                max_pairs_per_query=1000\n",
        "            )  # Gi·ªõi h·∫°n ƒë·ªÉ t√≠nh nhanh\n",
        "        accuracy = calculate_pairwise_accuracy(model, test_pairwise_data, sentence_model, device)\n",
        "\n",
        "        print(f\"\\nPairwise Accuracy: {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"HO√ÄN TH√ÄNH!\")\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "\n"
      ],
      "metadata": {
        "id": "LpgjMrNrMpLx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b47d8ad2-f1f3-4ec8-f895-1b28e2257892"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng device: cuda\n",
            "1. Load d·ªØ li·ªáu pairwise t·ª´ /content/document_pairs.json...\n",
            "ƒê√£ load 1332 c·∫∑p d·ªØ li·ªáu t·ª´ /content/document_pairs.json\n",
            "   ‚úì ƒê√£ load 1332 c·∫∑p t·ª´ file c√≥ s·∫µn\n",
            "   ‚úì Metadata: {'total_pairs': 1332, 'created_by': 'DataPreparator', 'format_version': '1.0', 'documents_path': '/content/cadao_tucngu_50_1.json', 'num_queries': 2, 'queries': ['Anh em nh∆∞ th·ªÉ tay ch√¢n', 'Anh em thu·∫≠n h√≤a'], 'relevance_method': 'cosine_ranking', 'sentence_model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2', 'max_pairs_per_query': 1000}\n",
            "ƒêang chu·∫©n b·ªã dataset...\n",
            "ƒêang kh·ªüi t·∫°o m√¥ h√¨nh RankNet...\n",
            "Input size: 768\n",
            "B·∫Øt ƒë·∫ßu hu·∫•n luy·ªán m√¥ h√¨nh...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:41<00:00,  1.01it/s, Loss=0.6480, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10, Average Loss: 0.6386\n",
            "New best model saved with loss: 0.6386\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:40<00:00,  1.04it/s, Loss=0.5373, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10, Average Loss: 0.5524\n",
            "New best model saved with loss: 0.5524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:40<00:00,  1.04it/s, Loss=0.4719, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3/10, Average Loss: 0.4960\n",
            "New best model saved with loss: 0.4960\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:41<00:00,  1.01it/s, Loss=0.4602, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4/10, Average Loss: 0.4671\n",
            "New best model saved with loss: 0.4671\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:40<00:00,  1.05it/s, Loss=0.5605, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5/10, Average Loss: 0.4508\n",
            "New best model saved with loss: 0.4508\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:40<00:00,  1.03it/s, Loss=0.4524, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6/10, Average Loss: 0.4304\n",
            "New best model saved with loss: 0.4304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:40<00:00,  1.03it/s, Loss=0.5322, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7/10, Average Loss: 0.4276\n",
            "New best model saved with loss: 0.4276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:41<00:00,  1.02it/s, Loss=0.5125, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8/10, Average Loss: 0.4181\n",
            "New best model saved with loss: 0.4181\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:40<00:00,  1.03it/s, Loss=0.4021, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9/10, Average Loss: 0.4154\n",
            "New best model saved with loss: 0.4154\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/10: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 42/42 [00:40<00:00,  1.05it/s, Loss=0.3956, LR=0.001000]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 10/10, Average Loss: 0.4068\n",
            "New best model saved with loss: 0.4068\n",
            "ƒê√£ l∆∞u m√¥ h√¨nh v√†o ranknet_model.pth\n",
            "\n",
            "==================================================\n",
            "ƒê√ÅNH GI√Å M√î H√åNH\n",
            "==================================================\n",
            "ƒêang ƒë√°nh gi√° ranking...\n",
            "\n",
            "Query: 'Anh em nh∆∞ th·ªÉ tay ch√¢n'\n",
            "Top 10 documents:\n",
            " 1. [ID: 33] Score: 4.3795\n",
            "    Text: Anh em nh∆∞ th·ªÉ tay ch√¢n....\n",
            "\n",
            " 2. [ID: 256] Score: 4.2375\n",
            "    Text: Ch·ªã em d√¢u nh∆∞ b·∫ßu n∆∞·ªõc l√£...\n",
            "\n",
            " 3. [ID: 6] Score: 4.1990\n",
            "    Text: Anh em nh∆∞ th·ªÉ ch√¢n tay R√°ch l√†nh ƒë√πm b·ªçc, d·ªü hay ƒë·ª° ƒë·∫ßn...\n",
            "\n",
            " 4. [ID: 60] Score: 4.1346\n",
            "    Text: Anh em nh∆∞ ch√¢n v·ªõi tay R√°ch l√†nh ƒë√πm b·ªçc, d·ªü hay ƒë·ª° ƒë·∫ßn....\n",
            "\n",
            " 5. [ID: 683] Score: 4.0680\n",
            "    Text: Nhi√™ÃÉu ƒëi√™ÃÄu phuÃâ l√¢ÃÅy giaÃÅ g∆∞∆°ng ChaÃ£y xe nh∆∞∆°ÃÄng nhiÃ£n, laÃÄ th∆∞∆°ng chiÃÅnh miÃÄnh...\n",
            "\n",
            " 6. [ID: 768] Score: 3.6605\n",
            "    Text: Th∆∞∆°ng ng∆∞·ªùi nh∆∞ th·ªÉ th∆∞∆°ng th√¢n....\n",
            "\n",
            " 7. [ID: 83] Score: 3.4763\n",
            "    Text: Anh em ƒÉn ∆°Ãâ thu√¢Ã£n hoaÃÄ Ch∆°ÃÅ ƒëi√™ÃÄu ch√™ÃÅch l√™Ã£ch ng∆∞∆°ÃÄi ta ch√™ c∆∞∆°ÃÄi...\n",
            "\n",
            " 8. [ID: 697] Score: 2.9633\n",
            "    Text: Nh∆∞∆°ÃÄng nhau kh√¥ng phaÃâi laÃÄ heÃÄn Nh∆∞∆°ÃÄng nhau ƒë√™Ãâ khoÃâi laÃÅch, leÃÄn, keÃ£t xe...\n",
            "\n",
            " 9. [ID: 75] Score: 2.8733\n",
            "    Text: Anh em nh∆∞ th√™Ãâ ch√¢n tay CuÃÄng cha cuÃÄng meÃ£ vi·ªác nh√† hƒÉng say...\n",
            "\n",
            "10. [ID: 647] Score: 1.9389\n",
            "    Text: Nh·∫•t c·∫≠n th·ªã, nh√¨ c·∫≠n l√¢n...\n",
            "\n",
            "ƒêang t√≠nh Pairwise Accuracy...\n",
            "============================================================\n",
            "CHU·∫®N B·ªä D·ªÆ LI·ªÜU PAIRWISE CHO RANKNET\n",
            "============================================================\n",
            "1. ƒêang load documents t·ª´ /content/cadao_tucngu_50_1.json...\n",
            "   ‚úì ƒê√£ load 49 documents\n",
            "2. ƒêang kh·ªüi t·∫°o sentence transformer: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2...\n",
            "   ‚úì Model ƒë√£ s·∫µn s√†ng\n",
            "3. ƒêang kh·ªüi t·∫°o DataPreparator...\n",
            "   ‚úì DataPreparator ƒë√£ s·∫µn s√†ng\n",
            "4. ƒêang t·∫°o d·ªØ li·ªáu pairwise...\n",
            "B·∫Øt ƒë·∫ßu t·∫°o d·ªØ li·ªáu pairwise v·ªõi 1 queries v√† 49 documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "T·∫°o d·ªØ li·ªáu pairwise: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00,  1.04it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Th·ªëng k√™ d·ªØ li·ªáu pairwise:\n",
            "T·ªïng s·ªë c·∫∑p: 666\n",
            "Label 0.0 (doc1 < doc2): 0 (0.0%)\n",
            "Label 0.5 (doc1 = doc2): 565 (84.8%)\n",
            "Label 1.0 (doc1 > doc2): 611 (91.7%)\n",
            "   ‚úì ƒê√£ t·∫°o 666 c·∫∑p d·ªØ li·ªáu\n",
            "5. ƒêang l∆∞u v√†o /content/document_pairs.json...\n",
            "ƒê√£ l∆∞u 666 c·∫∑p d·ªØ li·ªáu v√†o /content/document_pairs.json\n",
            "   ‚úì D·ªØ li·ªáu ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng\n",
            "============================================================\n",
            "HO√ÄN TH√ÄNH CHU·∫®N B·ªä D·ªÆ LI·ªÜU!\n",
            "============================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "T√≠nh Pairwise Accuracy: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 666/666 [00:21<00:00, 31.65it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Pairwise Accuracy: 0.9790 (97.90%)\n",
            "\n",
            "==================================================\n",
            "HO√ÄN TH√ÄNH!\n",
            "==================================================\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ti·∫øn h√†nh ki·ªÉm th·ª≠ m√¥ h√¨nh [RankNet] ƒë√£ ƒë∆∞·ª£c hu·∫•n luy·ªán, ch√∫ng ta s·∫Ω ki·ªÉm tra ƒë∆°n gi·∫£n b·∫±ng c√°ch - ƒë·∫ßu v√†o ch√∫ng ta s·∫Ω cho 2 t√†i li·ªáu ( di ) v√† ( dj ) (·ªü d·∫°ng 2 vectors)\n",
        "\n",
        "Sau ƒë√≥ ch√∫ng ta s·∫Ω d√πng m√¥ h√¨nh [RankNet] ƒë·ªÉ d·ª± ƒëo√°n th·ª≠ x√°c su·∫•t t√†i li·ªáu ( di ) li√™n quan v·ªõi truy v·∫•n ( q ) nhi·ªÅu h∆°n ( dj )"
      ],
      "metadata": {
        "id": "dhEk7slbCqBz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    try:\n",
        "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "        print(f\"S·ª≠ d·ª•ng device: {device}\")\n",
        "\n",
        "        # Load documents\n",
        "        documents_path = \"/content/cadao_tucngu_50_1.json\"\n",
        "        model_path = \"/content/ranknet_model.pth\"\n",
        "\n",
        "        with open(documents_path, 'r', encoding='utf-8') as f:\n",
        "            documents = json.load(f)\n",
        "\n",
        "        # Kh·ªüi t·∫°o models\n",
        "        sentence_model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
        "        preparator = DataPreparator(sentence_model)\n",
        "\n",
        "        # Load RankNet model\n",
        "        sample_query_emb = sentence_model.encode([\"test\"])[0]\n",
        "        input_size = len(sample_query_emb) * 2\n",
        "\n",
        "        if os.path.exists(model_path):\n",
        "            checkpoint = torch.load(model_path, map_location=device)\n",
        "            fc1_weight_shape = checkpoint['fc1.weight'].shape\n",
        "            fc2_weight_shape = checkpoint['fc2.weight'].shape\n",
        "            hidden_size1 = fc1_weight_shape[0]\n",
        "            hidden_size2 = fc2_weight_shape[0]\n",
        "\n",
        "            model = RankNet(input_size=input_size, hidden_size1=hidden_size1,\n",
        "                          hidden_size2=hidden_size2, dropout=0.5)\n",
        "            model.load_state_dict(checkpoint)\n",
        "            model.to(device)\n",
        "            print(f\"‚úÖ ƒê√£ load model t·ª´ {model_path}\")\n",
        "        else:\n",
        "            print(f\"‚ùå Kh√¥ng t√¨m th·∫•y model file: {model_path}\")\n",
        "            exit()\n",
        "\n",
        "        # TEST C·ª§ TH·ªÇ\n",
        "        query = \"B√°n anh em xa\"\n",
        "        doc_id1 = 75\n",
        "        doc_id2 = 107\n",
        "\n",
        "        test_specific_document_pair(\n",
        "            model=model,\n",
        "            sentence_model=sentence_model,\n",
        "            preparator=preparator,\n",
        "            documents=documents,\n",
        "            query=query,\n",
        "            doc_id1=doc_id1,\n",
        "            doc_id2=doc_id2,\n",
        "            device=device\n",
        "        )\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0WlKuK5xCuSu",
        "outputId": "3391fd61-be40-4c2a-914d-cc5217274691"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "S·ª≠ d·ª•ng device: cuda\n",
            "‚úÖ ƒê√£ load model t·ª´ /content/ranknet_model.pth\n",
            "============================================================\n",
            "TEST C·∫∂P T√ÄI LI·ªÜU C·ª§ TH·ªÇ\n",
            "============================================================\n",
            "üìÑ Query: 'B√°n anh em xa'\n",
            "üìÑ Document 1 [ID: 75]: 'Anh em nh∆∞ th√™Ãâ ch√¢n tay CuÃÄng cha cuÃÄng meÃ£ vi·ªác nh√† hƒÉng say'\n",
            "üìÑ Document 2 [ID: 107]: 'B√°n anh em xa, mua l√°ng gi·ªÅng g·∫ßn'\n",
            "\n",
            "üîß Preprocessed query: 'b√°n anh_em xa'\n",
            "üîß Preprocessed doc1: 'anh_em nh∆∞ th√™ ch√¢n tay cu ng cha cu ng me vi·ªác nh√† hƒÉng_say'\n",
            "üîß Preprocessed doc2: 'b√°n anh_em xa mua l√°ng_gi·ªÅng g·∫ßn'\n",
            "üîß Feature vector size: 768\n",
            "\n",
            "üìä K·∫æT QU·∫¢:\n",
            "   Score Document 1: 4.1221\n",
            "   Score Document 2: -0.8170\n",
            "   Score difference: 4.9391\n",
            "   P(Doc1 > Doc2): 0.9929 (99.3%)\n",
            "\n",
            "üéØ K·∫æT LU·∫¨N: Document 1 (ID: 75) c√≥ kh·∫£ nƒÉng cao h∆°n li√™n quan v·ªõi query\n",
            "\n",
            "üìà SO S√ÅNH V·ªöI COSINE SIMILARITY:\n",
            "   Query vs Doc1: 0.6656\n",
            "   Query vs Doc2: 0.8163\n",
            "   Theo cosine similarity: Document 2 li√™n quan h∆°n\n",
            "   ‚ö†Ô∏è  Model v√† Cosine similarity c√≥ k·∫øt qu·∫£ kh√°c nhau\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}